{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/rsg/chemistry/johnyang/miniconda3/envs/cot2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjohnyang\u001b[0m (\u001b[33mprotein-optimization\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Mounts/rbg-storage1/users/johnyang/cellot/notebooks/wandb/run-20230717_110708-7do2mivk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/protein-optimization/cellot-notebooks/runs/7do2mivk' target=\"_blank\">copper-pond-14</a></strong> to <a href='https://wandb.ai/protein-optimization/cellot-notebooks' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/protein-optimization/cellot-notebooks' target=\"_blank\">https://wandb.ai/protein-optimization/cellot-notebooks</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/protein-optimization/cellot-notebooks/runs/7do2mivk' target=\"_blank\">https://wandb.ai/protein-optimization/cellot-notebooks/runs/7do2mivk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "run = wandb.init()\n",
    "artifact = run.use_artifact('protein-optimization/sc_diff/model-lozzw8vu:v55', type='model')\n",
    "artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./artifacts/model-lozzw8vu:v55'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artifact_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cellot.models.cond_score_module import CondScoreModuleV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = f'{artifact_dir}/model.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "YAML_STR = '''\n",
    "DEBUG: False\n",
    "TARGET: trametinib\n",
    "LATENT_DIM: 50\n",
    "COND_CLASSES: 190\n",
    "SEED: 42\n",
    "AE_PATH: /Mounts/rbg-storage1/users/johnyang/cellot/results/sciplex3/full_ae\n",
    "VAL_SIZE: 0.1\n",
    "DEVICES: 1\n",
    "WARM_START: False\n",
    "WARM_START_PATH: null\n",
    "MODEL_CLASS: CondScoreModule\n",
    "\n",
    "\n",
    "diffuser:\n",
    "  min_b: 0.01\n",
    "  max_b: 1.0\n",
    "  schedule: exponential\n",
    "  score_scaling: var\n",
    "  coordinate_scaling: 1.0\n",
    "  latent_dim: ${LATENT_DIM}\n",
    "  dt: 0.01\n",
    "  min_t: 0\n",
    "\n",
    "ae:\n",
    "  name: scgen\n",
    "  beta: 0.0\n",
    "  dropout: 0.0\n",
    "  hidden_units: [512, 512]\n",
    "  latent_dim: 50\n",
    "\n",
    "score_network:\n",
    "  latent_dim: ${LATENT_DIM}\n",
    "  cond_classes: ${COND_CLASSES}\n",
    "  model_dim: 256   # Adjusted to 64\n",
    "  input_dim: 256\n",
    "  n_layers: 3    # Adjusted to 12\n",
    "  # nhead: 8\n",
    "  # dim_feedforward: 2048\n",
    "  dropout: 0.1\n",
    "  # ffn_hidden_dim: 1024\n",
    "  final_layers: 64\n",
    "  extra_null_cond_embedding: False\n",
    "\n",
    "\n",
    "data:\n",
    "  type: cell\n",
    "  source: control\n",
    "  condition: drug\n",
    "  path: /Mounts/rbg-storage1/users/johnyang/cellot/datasets/scrna-sciplex3/hvg.h5ad\n",
    "  target: ${TARGET}\n",
    "\n",
    "datasplit:\n",
    "  groupby: drug   \n",
    "  name: train_test\n",
    "  test_size: 0.2\n",
    "  random_state: 0\n",
    "  \n",
    "dataloader:\n",
    "  batch_size: 256   # Adjusted to 256\n",
    "  shuffle: true\n",
    "  num_workers: 80\n",
    "  \n",
    "experiment:\n",
    "  name: base\n",
    "  mode: train\n",
    "  num_loader_workers: 0\n",
    "  port: 12319\n",
    "  dist_mode: single\n",
    "  use_wandb: True\n",
    "  ckpt_path: null\n",
    "  wandb_logger:\n",
    "    project: sc_diff\n",
    "    name: ${experiment.name}\n",
    "    dir: /Mounts/rbg-storage1/users/johnyang/cellot/\n",
    "    log_model: all\n",
    "    tags: ['experimental']\n",
    "  lr: 0.0001\n",
    "\n",
    "\n",
    "trainer:\n",
    "  accelerator: 'gpu'\n",
    "  check_val_every_n_epoch: 50\n",
    "  log_every_n_steps: 100\n",
    "  num_sanity_val_steps: 1\n",
    "  enable_progress_bar: True\n",
    "  enable_checkpointing: True\n",
    "  fast_dev_run: False\n",
    "  profiler: simple\n",
    "  max_epochs: 10000\n",
    "  strategy: auto\n",
    "  enable_model_summary: True\n",
    "  overfit_batches: 0.0\n",
    "  limit_train_batches: 1.0\n",
    "  limit_val_batches: 1.0\n",
    "  limit_predict_batches: 1.0\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "config = OmegaConf.create(YAML_STR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = f'{artifact_dir}/model.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Using GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "from cellot.train.utils import get_free_gpu\n",
    "replica_id = int(get_free_gpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = f'cuda:{replica_id}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DEBUG': False, 'TARGET': 'trametinib', 'LATENT_DIM': 50, 'COND_CLASSES': 190, 'SEED': 42, 'AE_PATH': '/Mounts/rbg-storage1/users/johnyang/cellot/results/sciplex3/full_ae', 'VAL_SIZE': 0.1, 'DEVICES': 1, 'WARM_START': False, 'WARM_START_PATH': None, 'MODEL_CLASS': 'CondScoreModule', 'diffuser': {'min_b': 0.01, 'max_b': 1.0, 'schedule': 'exponential', 'score_scaling': 'var', 'coordinate_scaling': 1.0, 'latent_dim': '${LATENT_DIM}', 'dt': 0.01, 'min_t': 0}, 'ae': {'name': 'scgen', 'beta': 0.0, 'dropout': 0.0, 'hidden_units': [512, 512], 'latent_dim': 50}, 'score_network': {'latent_dim': '${LATENT_DIM}', 'cond_classes': '${COND_CLASSES}', 'model_dim': 256, 'n_layers': 6, 'nhead': 8, 'dim_feedforward': 2048, 'dropout': 0.1, 'ffn_hidden_dim': 1024, 'extra_null_cond_embedding': False}, 'data': {'type': 'cell', 'source': 'control', 'condition': 'drug', 'path': '/Mounts/rbg-storage1/users/johnyang/cellot/datasets/scrna-sciplex3/hvg.h5ad', 'target': '${TARGET}'}, 'datasplit': {'groupby': 'drug', 'name': 'train_test', 'test_size': 0.2, 'random_state': 0}, 'dataloader': {'batch_size': 256, 'shuffle': True, 'num_workers': 80}, 'experiment': {'name': 'base', 'mode': 'train', 'num_loader_workers': 0, 'port': 12319, 'dist_mode': 'single', 'use_wandb': True, 'ckpt_path': None, 'wandb_logger': {'project': 'sc_diff', 'name': '${experiment.name}', 'dir': '/Mounts/rbg-storage1/users/johnyang/cellot/', 'log_model': 'all', 'tags': ['experimental']}, 'lr': 0.0001}, 'trainer': {'accelerator': 'gpu', 'check_val_every_n_epoch': 50, 'log_every_n_steps': 100, 'num_sanity_val_steps': 1, 'enable_progress_bar': True, 'enable_checkpointing': True, 'fast_dev_run': False, 'profiler': 'simple', 'max_epochs': 10000, 'strategy': 'auto', 'enable_model_summary': True, 'overfit_batches': 0.0, 'limit_train_batches': 1.0, 'limit_val_batches': 1.0, 'limit_predict_batches': 1.0}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropout is 0.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lm = CondScoreModuleV2.load_from_checkpoint(hparams=config, checkpoint_path=ckpt_path).to(device)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from checkpoint\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import cellot.models\n",
    "from cellot.data.cell import load_cell_data\n",
    "import torch\n",
    "from cellot.models.ae import AutoEncoder\n",
    "from pathlib import Path\n",
    "\n",
    "def load_data(config, **kwargs):\n",
    "    data_type = config.get(\"data.type\", \"cell\")\n",
    "    if data_type in [\"cell\", \"cell-merged\", \"tupro-cohort\"]:\n",
    "        loadfxn = load_cell_data\n",
    "\n",
    "    elif data_type == \"toy\":\n",
    "        loadfxn = load_toy_data\n",
    "\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    return loadfxn(config, **kwargs)\n",
    "\n",
    "\n",
    "def load_model(config, device, restore=None, **kwargs):\n",
    "    # def load_autoencoder_model(config, restore=None, **kwargs):\n",
    "    \n",
    "    def load_optimizer(config, params):\n",
    "        kwargs = dict(config.get(\"optim\", {}))\n",
    "        assert kwargs.pop(\"optimizer\", \"Adam\") == \"Adam\"\n",
    "        optim = torch.optim.Adam(params, **kwargs)\n",
    "        return optim\n",
    "\n",
    "\n",
    "    def load_networks(config, **kwargs):\n",
    "        kwargs = kwargs.copy()\n",
    "        kwargs.update(dict(config.get(\"ae\", {})))\n",
    "        name = kwargs.pop(\"name\")\n",
    "\n",
    "        # if name == \"scgen\":\n",
    "        model = AutoEncoder\n",
    "\n",
    "        # elif name == \"cae\":\n",
    "        #     model = ConditionalAutoEncoder\n",
    "        # else:\n",
    "        #     raise ValueError\n",
    "\n",
    "        return model(**kwargs)\n",
    "    \n",
    "    model = load_networks(config, **kwargs)\n",
    "    optim = load_optimizer(config, model.parameters())\n",
    "\n",
    "    if restore is not None and Path(restore).exists():\n",
    "        print('Loading model from checkpoint')\n",
    "        ckpt = torch.load(restore, map_location=device)\n",
    "        model.load_state_dict(ckpt[\"model_state\"])\n",
    "        optim.load_state_dict(ckpt[\"optim_state\"])\n",
    "        # if config.model.name == \"scgen\" and \"code_means\" in ckpt:\n",
    "        #     model.code_means = ckpt[\"code_means\"]\n",
    "            \n",
    "    # logger.info(f'Model on device {next(model.parameters()).device}')\n",
    "\n",
    "    return model, optim\n",
    "\n",
    "def load(config, device, restore=None, include_model_kwargs=False, **kwargs):\n",
    "\n",
    "    loader, model_kwargs = load_data(config, include_model_kwargs=True, **kwargs)\n",
    "\n",
    "    model, opt = load_model(config, device, restore=restore, **model_kwargs)\n",
    "\n",
    "    return model, opt, loader\n",
    "# %% [markdown]\n",
    "# ### Training\n",
    "\n",
    "# %%\n",
    "restore_path = '/Mounts/rbg-storage1/users/johnyang/cellot/saved_weights/ae/ae.pt'\n",
    "ae, _ = load_model(config, 'cuda', restore=restore_path, input_dim=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = ae.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import gamma\n",
    "import torch\n",
    "\n",
    "\n",
    "class LatentDiffuser:\n",
    "    \"\"\"VP-SDE diffuser class for translations.\"\"\"\n",
    "\n",
    "    def __init__(self, conf):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            min_b: starting value in variance schedule.\n",
    "            max_b: ending value in variance schedule.\n",
    "        \"\"\"\n",
    "        self._conf = conf\n",
    "        self.min_b = conf.min_b\n",
    "        self.max_b = conf.max_b\n",
    "        self.schedule = conf.schedule\n",
    "        self._score_scaling = conf.score_scaling\n",
    "        self.latent_dim = conf.latent_dim\n",
    "\n",
    "    def _scale(self, x):\n",
    "        return x * self._conf.coordinate_scaling\n",
    "\n",
    "    def _unscale(self, x):\n",
    "        return x / self._conf.coordinate_scaling\n",
    "\n",
    "    def b_t(self, t):\n",
    "        if np.any(t < 0) or np.any(t > 1):\n",
    "            raise ValueError(f'Invalid t={t}')\n",
    "        if self.schedule == 'linear': \n",
    "            return self.min_b + t*(self.max_b - self.min_b)\n",
    "        elif self.schedule == 'cosine':\n",
    "            return self.max_b + 0.5*(self.min_b - self.max_b)*(1 + np.cos(t*np.pi))\n",
    "        elif self.schedule == 'exponential':\n",
    "            sigma = t * np.log10(self.max_b) + (1 - t) * np.log10(self.min_b)\n",
    "            return 10 ** sigma\n",
    "        else:\n",
    "            raise ValueError(f'Unknown schedule {self.schedule}')\n",
    "    \n",
    "    def diffusion_coef(self, t):\n",
    "        \"\"\"Time-dependent diffusion coefficient.\"\"\"\n",
    "        return np.sqrt(self.b_t(t))\n",
    "\n",
    "    def drift_coef(self, x, t):\n",
    "        \"\"\"Time-dependent drift coefficient.\"\"\"\n",
    "        return -1/2 * self.b_t(t) * x\n",
    "\n",
    "    def sample_ref(self, n_samples: float=1):\n",
    "        return np.random.normal(size=(n_samples, self.latent_dim))\n",
    "\n",
    "    def marginal_b_t(self, t):\n",
    "        if self.schedule == 'linear':\n",
    "            return t*self.min_b + (1/2)*(t**2)*(self.max_b-self.min_b)\n",
    "        elif self.schedule == 'exponential': \n",
    "            return (self.max_b**t * self.min_b**(1-t) - self.min_b) / (\n",
    "                np.log(self.max_b) - np.log(self.min_b))\n",
    "        else:\n",
    "            raise ValueError(f'Unknown schedule {self.schedule}')\n",
    "\n",
    "    def calc_trans_0(self, score_t, x_t, t, use_torch=True):\n",
    "        beta_t = self.marginal_b_t(t)\n",
    "        beta_t = beta_t[..., None, None]\n",
    "        exp_fn = torch.exp if use_torch else np.exp\n",
    "        cond_var = 1 - exp_fn(-beta_t)\n",
    "        return (score_t * cond_var + x_t) / exp_fn(-1/2*beta_t)\n",
    "\n",
    "    def forward(self, x_t_1: np.ndarray, t: float, num_t: int):\n",
    "        \"\"\"Samples marginal p(x(t) | x(t-1)).\n",
    "\n",
    "        Args:\n",
    "            x_0: [..., n, 3] initial positions in Angstroms.\n",
    "            t: continuous time in [0, 1]. \n",
    "\n",
    "        Returns:\n",
    "            x_t: [..., n, 3] positions at time t in Angstroms.\n",
    "            score_t: [..., n, 3] score at time t in scaled Angstroms.\n",
    "        \"\"\"\n",
    "        if not np.isscalar(t):\n",
    "            raise ValueError(f'{t} must be a scalar.')\n",
    "        x_t_1 = self._scale(x_t_1)\n",
    "        b_t = torch.tensor(self.marginal_b_t(t) / num_t).to(x_t_1.device)\n",
    "        z_t_1 = torch.tensor(np.random.normal(size=x_t_1.shape)).to(x_t_1.device)\n",
    "        x_t = torch.sqrt(1 - b_t) * x_t_1 + torch.sqrt(b_t) * z_t_1\n",
    "        return x_t\n",
    "    \n",
    "    def distribution(self, x_t, score_t, t, mask, dt):\n",
    "        x_t = self._scale(x_t)\n",
    "        g_t = self.diffusion_coef(t)\n",
    "        f_t = self.drift_coef(x_t, t)\n",
    "        std = g_t * np.sqrt(dt)\n",
    "        mu = x_t - (f_t - g_t**2 * score_t) * dt\n",
    "        if mask is not None:\n",
    "            mu *= mask[..., None]\n",
    "        return mu, std\n",
    "\n",
    "    def forward_marginal(self, x_0: np.ndarray, t: float):\n",
    "        \"\"\"Samples marginal p(x(t) | x(0)).\n",
    "\n",
    "        Args:\n",
    "            x_0: [..., n, 3] initial positions in Angstroms.\n",
    "            t: continuous time in [0, 1]. \n",
    "\n",
    "        Returns:\n",
    "            x_t: [..., n, 3] positions at time t in Angstroms.\n",
    "            score_t: [..., n, 3] score at time t in scaled Angstroms.\n",
    "        \"\"\"\n",
    "        if not np.isscalar(t):\n",
    "            raise ValueError(f'{t} must be a scalar.')\n",
    "        x_0 = self._scale(x_0)\n",
    "        x_t = np.random.normal(\n",
    "            loc=np.exp(-1/2*self.marginal_b_t(t)) * x_0,\n",
    "            scale=np.sqrt(1 - np.exp(-self.marginal_b_t(t)))\n",
    "        )\n",
    "        score_t = self.score(x_t, x_0, t)\n",
    "        x_t = self._unscale(x_t)\n",
    "        return x_t, score_t\n",
    "\n",
    "    def score_scaling(self, t: float):\n",
    "        if self._score_scaling == 'var':\n",
    "            return 1 / self.conditional_var(t)\n",
    "        elif self._score_scaling == 'std':\n",
    "            return 1 / np.sqrt(self.conditional_var(t))\n",
    "        elif self._score_scaling == 'expected_norm':\n",
    "            return np.sqrt(2) / (gamma(1.5) * np.sqrt(self.conditional_var(t)))\n",
    "        else:\n",
    "            raise ValueError(f'Unrecognized scaling {self._score_scaling}')\n",
    "        \n",
    "    def ode(self,\n",
    "            *,\n",
    "            x_t: np.ndarray,\n",
    "            score_t: np.ndarray,\n",
    "            t: float,\n",
    "            dt: float,\n",
    "            mask: np.ndarray=None,\n",
    "            noise_scale: float=1.0,\n",
    "        ):\n",
    "        if not np.isscalar(t):\n",
    "            raise ValueError(f'{t} must be a scalar.')\n",
    "        x_t = self._scale(x_t)\n",
    "        g_t = self.diffusion_coef(t)\n",
    "        f_t = self.drift_coef(x_t, t)\n",
    "\n",
    "        # Probability flow ODE\n",
    "        perturb = (f_t - (1/2)*(g_t**2) * score_t) * dt\n",
    "        \n",
    "        if mask is not None:\n",
    "            perturb *= mask[..., None]\n",
    "        else:\n",
    "            mask = np.ones(x_t.shape[:-1])\n",
    "        \n",
    "        # For positive dt, we add dx and vv.\n",
    "        x_t_1 = x_t + perturb\n",
    "        x_t_1 = self._unscale(x_t_1)\n",
    "        return x_t_1\n",
    "        \n",
    "    def reverse(\n",
    "            self,\n",
    "            *,\n",
    "            x_t: np.ndarray,\n",
    "            score_t: np.ndarray,\n",
    "            t: float,\n",
    "            dt: float,\n",
    "            mask: np.ndarray=None,\n",
    "            center: bool=True,\n",
    "            ode: bool=False,\n",
    "            noise_scale: float=1.0,\n",
    "        ):\n",
    "        \"\"\"Simulates the reverse SDE for 1 step\n",
    "\n",
    "        Args:\n",
    "            x_t: [..., 3] current positions at time t in angstroms.\n",
    "            score_t: [..., 3] rotation score at time t.\n",
    "            t: continuous time in [0, 1].\n",
    "            dt: continuous step size in [0, 1].\n",
    "            mask: True indicates which residues to diffuse.\n",
    "\n",
    "        Returns:\n",
    "            [..., 3] positions at next step t-1.\n",
    "        \"\"\"\n",
    "        if not np.isscalar(t):\n",
    "            raise ValueError(f'{t} must be a scalar.')\n",
    "        x_t = self._scale(x_t)\n",
    "        g_t = self.diffusion_coef(t)\n",
    "        f_t = self.drift_coef(x_t, t)\n",
    "        if ode:\n",
    "            # Probability flow ODE\n",
    "            perturb = (f_t - (1/2)*(g_t**2) * score_t) * dt\n",
    "        else:\n",
    "            # Usual stochastic dynamics\n",
    "            z = noise_scale * np.random.normal(size=score_t.shape)\n",
    "            perturb = (f_t - g_t**2 * score_t) * dt + g_t * np.sqrt(dt) * z\n",
    "\n",
    "        if mask is not None:\n",
    "            perturb *= mask[..., None]\n",
    "        else:\n",
    "            mask = np.ones(x_t.shape[:-1])\n",
    "        x_t_1 = x_t - perturb\n",
    "        if center:\n",
    "            com = np.sum(x_t_1, axis=-2) / np.sum(mask, axis=-1)[..., None]\n",
    "            x_t_1 -= com[..., None, :]\n",
    "        x_t_1 = self._unscale(x_t_1)\n",
    "        return x_t_1\n",
    "\n",
    "    def conditional_var(self, t, use_torch=False):\n",
    "        \"\"\"Conditional variance of p(xt|x0).\n",
    "\n",
    "        Var[x_t|x_0] = conditional_var(t)*I\n",
    "\n",
    "        \"\"\"\n",
    "        if use_torch:\n",
    "            return 1 - torch.exp(-self.marginal_b_t(t))\n",
    "        return 1 - np.exp(-self.marginal_b_t(t))\n",
    "\n",
    "    def score(self, x_t, x_0, t, use_torch=False, scale=False):\n",
    "        if use_torch:\n",
    "            exp_fn = torch.exp\n",
    "        else:\n",
    "            exp_fn = np.exp\n",
    "        if scale:\n",
    "            x_t = self._scale(x_t)\n",
    "            x_0 = self._scale(x_0)\n",
    "        return -(x_t - exp_fn(-1/2*self.marginal_b_t(t)) * x_0) / self.conditional_var(t, use_torch=use_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffuser = LatentDiffuser(\n",
    "    config.diffuser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from omegaconf import DictConfig\n",
    "import hydra, torch, numpy as np\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger, WandbLogger\n",
    "from cellot.models.cond_score_module import CondScoreModule, CondScoreModuleV2\n",
    "from cellot.data.sciplex_ae_dm import CellDataModule\n",
    "from cellot.data.utils import load_ae_cell_data, load_ae, cast_dataset_to_loader\n",
    "from cellot.train.utils import get_free_gpu\n",
    "from cellot.utils.dev_utils import load_markers, compute_mmd_loss, get_ckpt_path_from_artifact_id, get_target_cond_idx\n",
    "from tqdm import tqdm\n",
    "\n",
    "gammas = np.logspace(1, -3, num=50)\n",
    "\n",
    "def inference(lm, batch, lamb=4, dt=0.01, t_start=1.0, cond=True, ae=None, target=None):\n",
    "    device = lm.device\n",
    "    assert ae is not None, 'Must provide autoencoder'\n",
    "    assert target is not None or not cond, 'Must provide target'\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        lm.eval()\n",
    "        all_genes_x, y_batch = batch\n",
    "        \n",
    "        if cond:\n",
    "            y = torch.ones_like(y_batch) * get_target_cond_idx(target)\n",
    "        else:\n",
    "            y = y_batch\n",
    "        \n",
    "        y = y.to(device)\n",
    "        \n",
    "        latent_x = ae.eval().encode(all_genes_x)\n",
    "        latent_iden_recon = ae.eval().decode(latent_x)\n",
    "        \n",
    "        x_t, _ = diffuser.forward_marginal(latent_x.detach().cpu().numpy(), t=t_start)\n",
    "        \n",
    "        for i, t in enumerate(np.arange(t_start, 0, -dt)):\n",
    "            x_t = torch.tensor(x_t).float().to(device)\n",
    "            uncond_score = lm.score_network((x_t, (torch.ones_like(y) * lm.score_network.null_cond_idx).to(device)), t)\n",
    "            if cond:\n",
    "                cond_score = lm.score_network((x_t, y), t)\n",
    "                pred_score = (1 + lamb) * cond_score - lamb * uncond_score\n",
    "            else:\n",
    "                pred_score = uncond_score\n",
    "            \n",
    "            x_t = diffuser.ode(x_t=x_t.detach().cpu().numpy(), score_t=pred_score.detach().cpu().numpy(), t=t, dt=lm.dt)\n",
    "        \n",
    "        x_0 = torch.tensor(x_t, dtype=torch.float).to(lm.device)\n",
    "        \n",
    "        recon = ae.eval().decode(x_0)\n",
    "        return recon, latent_x, x_0, latent_iden_recon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-17 11:08:53,793 Loaded cell data with TARGET trametinib and OBS SHAPE (20842, 16)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['ENSG00000198074.9', 'ENSG00000019186.9', 'ENSG00000108846.15',\n",
       "       'ENSG00000115414.18', 'ENSG00000231185.6', 'ENSG00000112541.13',\n",
       "       'ENSG00000117983.17', 'ENSG00000145819.15', 'ENSG00000184588.17',\n",
       "       'ENSG00000165376.10', 'ENSG00000154529.14', 'ENSG00000182752.9',\n",
       "       'ENSG00000251003.7', 'ENSG00000101144.12', 'ENSG00000117724.12',\n",
       "       'ENSG00000157168.18', 'ENSG00000275395.5', 'ENSG00000185483.11',\n",
       "       'ENSG00000108405.3', 'ENSG00000089199.9', 'ENSG00000254166.2',\n",
       "       'ENSG00000215182.8', 'ENSG00000004948.13', 'ENSG00000227706.3',\n",
       "       'ENSG00000065809.13', 'ENSG00000004799.7', 'ENSG00000144847.12',\n",
       "       'ENSG00000107957.16', 'ENSG00000108602.17', 'ENSG00000059804.15',\n",
       "       'ENSG00000047648.21', 'ENSG00000076706.16', 'ENSG00000003436.15',\n",
       "       'ENSG00000229140.8', 'ENSG00000066279.17', 'ENSG00000153956.15',\n",
       "       'ENSG00000086548.8', 'ENSG00000171408.13', 'ENSG00000005108.15',\n",
       "       'ENSG00000138696.10', 'ENSG00000236213.1', 'ENSG00000038427.15',\n",
       "       'ENSG00000064042.17', 'ENSG00000130656.4', 'ENSG00000180287.16',\n",
       "       'ENSG00000204740.10', 'ENSG00000023171.16', 'ENSG00000153976.2',\n",
       "       'ENSG00000167281.18', 'ENSG00000113448.18'],\n",
       "      dtype='object', name='id')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cellot.utils.dev_utils import load_markers\n",
    "\n",
    "sel_mg, gene_idxs = load_markers(config)\n",
    "sel_mg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cellot.data.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DEV_load_ae_cell_data(\n",
    "        config,\n",
    "        data=None,\n",
    "        split_on=None,\n",
    "        return_as=\"loader\",\n",
    "        include_model_kwargs=False,\n",
    "        pair_batch_on=None,\n",
    "        ae=None,\n",
    "        encode_latents=False,\n",
    "        sel_mg=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        assert ae is not None or not encode_latents, \"ae must be provided\"\n",
    "        \n",
    "        if isinstance(return_as, str):\n",
    "            return_as = [return_as]\n",
    "\n",
    "        assert set(return_as).issubset({\"anndata\", \"dataset\", \"loader\"})\n",
    "        config.data.condition = config.data.get(\"condition\", \"drug\")\n",
    "        condition = config.data.condition\n",
    "        \n",
    "        data = read_single_anndata(config, **kwargs)\n",
    "        \n",
    "        inputs = torch.Tensor(\n",
    "            data.X if not sparse.issparse(data.X) else data.X.todense()\n",
    "        )\n",
    "\n",
    "        if encode_latents:\n",
    "            genes = data.var_names.to_list()\n",
    "            data = anndata.AnnData(\n",
    "                ae.eval().encode(inputs).detach().numpy(),\n",
    "                obs=data.obs.copy(),\n",
    "                uns=data.uns.copy(),\n",
    "            )\n",
    "            data.uns[\"genes\"] = genes\n",
    "\n",
    "\n",
    "        # cast to dense and check for nans\n",
    "        if sparse.issparse(data.X):\n",
    "            data.X = data.X.todense()\n",
    "        assert not np.isnan(data.X).any()\n",
    "\n",
    "        if sel_mg is not None:\n",
    "            data = data[:, sel_mg]\n",
    "\n",
    "        dataset_args = dict()\n",
    "        model_kwargs = {}\n",
    "\n",
    "        model_kwargs[\"input_dim\"] = data.n_vars\n",
    "\n",
    "        # if config.get(\"model.name\") == \"cae\":\n",
    "        condition_labels = sorted(data.obs[condition].cat.categories)\n",
    "        model_kwargs[\"conditions\"] = condition_labels\n",
    "        dataset_args[\"obs\"] = condition\n",
    "        dataset_args[\"categories\"] = condition_labels\n",
    "\n",
    "        if \"training\" in config:\n",
    "            pair_batch_on = config.training.get(\"pair_batch_on\", pair_batch_on)\n",
    "\n",
    "        # if split_on is None:\n",
    "            # if config.model.name == \"cellot\":\n",
    "            #     # datasets & dataloaders accessed as loader.train.source\n",
    "        split_on = [\"split\", \"transport\"]\n",
    "        if pair_batch_on is not None:\n",
    "            split_on.append(pair_batch_on)\n",
    "\n",
    "            # if (config.ae.name == \"scgen\" #or config.ae.name == \"cae\"\n",
    "            #     #or config.ae.name == \"popalign\"):\n",
    "            # split_on = [\"split\"]\n",
    "\n",
    "            # else:\n",
    "            #     raise ValueError\n",
    "\n",
    "        if isinstance(split_on, str):\n",
    "            split_on = [split_on]\n",
    "\n",
    "        for key in split_on:\n",
    "            assert key in data.obs.columns\n",
    "\n",
    "        if len(split_on) > 0:\n",
    "            splits = {\n",
    "                (key if isinstance(key, str) else \".\".join(key)): data[index]\n",
    "                for key, index in data.obs[split_on].groupby(split_on).groups.items()\n",
    "            }\n",
    "\n",
    "            dataset = nest_dict(\n",
    "                {\n",
    "                    key: AnnDataDataset(val.copy(), **dataset_args)\n",
    "                    for key, val in splits.items()\n",
    "                },\n",
    "                as_dot_dict=True,\n",
    "            )\n",
    "        else:\n",
    "            dataset = AnnDataDataset(data.copy(), **dataset_args)\n",
    "\n",
    "        if \"loader\" in return_as:\n",
    "            kwargs = dict(config.dataloader)\n",
    "            kwargs.setdefault(\"drop_last\", True)\n",
    "            loader = cast_dataset_to_loader(dataset, **kwargs)\n",
    "\n",
    "        returns = list()\n",
    "        for key in return_as:\n",
    "            if key == \"anndata\":\n",
    "                returns.append(data)\n",
    "\n",
    "            elif key == \"dataset\":\n",
    "                returns.append(dataset)\n",
    "\n",
    "            elif key == \"loader\":\n",
    "                returns.append(loader)\n",
    "\n",
    "        if include_model_kwargs:\n",
    "            returns.append(model_kwargs)\n",
    "\n",
    "        if len(returns) == 1:\n",
    "            return returns[0]\n",
    "\n",
    "        # returns.append(data)\n",
    "\n",
    "        return tuple(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = DEV_load_ae_cell_data(config, return_as='dataset')#, ae=autoencoder.cpu(), encode_latents=True)#, sel_mg=sel_mg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': {'source': <torch.utils.data.dataloader.DataLoader at 0x7efbc57854c0>,\n",
       "  'target': <torch.utils.data.dataloader.DataLoader at 0x7efbc5785f40>},\n",
       " 'train': {'source': <torch.utils.data.dataloader.DataLoader at 0x7efbc5785c70>,\n",
       "  'target': <torch.utils.data.dataloader.DataLoader at 0x7efbc57851f0>}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = cast_dataset_to_loader(datasets, batch_size=256, shuffle=False, drop_last=False)\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = datasets.test.source.adata.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = datasets.test.target.adata.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3513, 50)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source[:, gene_idxs[:50]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(656, 50)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[:, gene_idxs[:50]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cellot.losses.mmd import mmd_distance\n",
    "import numpy as np\n",
    "\n",
    "def compute_mmd_loss(lhs, rhs, gammas):\n",
    "    return np.mean([mmd_distance(lhs, rhs, g) for g in gammas])\n",
    "\n",
    "gammas = np.logspace(1, -3, num=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_target = target[:, gene_idxs[:50]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.80774903, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 2.6273441 , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoEncoder(\n",
       "  (encoder_net): Sequential(\n",
       "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=50, bias=True)\n",
       "  )\n",
       "  (decoder_net): Sequential(\n",
       "    (0): Linear(in_features=50, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=1000, bias=True)\n",
       "  )\n",
       "  (mse): MSELoss()\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cellot import losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [01:23<00:00,  5.99s/it]\n"
     ]
    }
   ],
   "source": [
    "gts = []\n",
    "recons = []\n",
    "lxs = []\n",
    "xs = []\n",
    "loss_list = []\n",
    "for batch in tqdm(loader.test.source):\n",
    "    batch = [x.to(device) for x in batch]\n",
    "    gts.append(batch)\n",
    "    recon, latent_x, recon_x_0, latent_iden_recon = inference(lm, batch, lamb=4, dt=0.01, t_start=1.0, cond=True, ae=ae, target=config.TARGET)\n",
    "    \n",
    "    loss_list.append(losses.compute_scalar_mmd(recon.detach().cpu().numpy(), sel_target, gammas))\n",
    "    recons.append(recon)\n",
    "    lxs.append(latent_x)\n",
    "    xs.append(recon_x_0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0], device='cuda:1')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(gts[0][0][0, gene_idxs[:50]] > 0.1, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,\n",
       "        1, 1], device='cuda:1')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(recons[0][0, gene_idxs[:50]] > 0.1, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3513, 1000])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_recon = torch.cat(recons, dim=0)\n",
    "all_recon.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lxs = torch.cat(lxs, dim=0)\n",
    "all_xs = torch.cat(xs, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 36.58it/s]\n"
     ]
    }
   ],
   "source": [
    "lx_targets = []\n",
    "with torch.inference_mode():\n",
    "    for batch in tqdm(loader.test.target):\n",
    "        batch = [x.to(device) for x in batch]\n",
    "        autoencoder.eval()\n",
    "        x, y = batch\n",
    "        latent_x_target = autoencoder.encode(x)\n",
    "        lx_targets.append(latent_x_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02164543247476104"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_mmd_loss(all_recon[:, gene_idxs[:50]].detach().cpu().numpy(), sel_target, gammas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02037286446526802"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cellot import losses\n",
    "losses.compute_scalar_mmd(all_recon[:, gene_idxs[:50]].detach().cpu().numpy(), sel_target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cellot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
