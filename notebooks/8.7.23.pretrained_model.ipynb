{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/rsg/chemistry/johnyang/miniconda3/envs/cot2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.functional import pad\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data import random_split\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from functools import reduce\n",
    "import math\n",
    "from einops import rearrange, repeat\n",
    "from perceiver_pytorch import PerceiverIO\n",
    "from torch import nn, einsum\n",
    "import json\n",
    "import glob\n",
    "\n",
    "import h5py as h5\n",
    "from functools import partial\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "VOCAB_SIZE = 60873\n",
    "\n",
    "MAX_LEN = 18976\n",
    "\n",
    "DEFAULT_ENCODING = \"utf-8\"\n",
    "\n",
    "DEFAULT_BINS = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_vocab(path):\n",
    "    with open(path, \"r\", encoding=DEFAULT_ENCODING) as inf:\n",
    "        return {gene: i for i, gene in enumerate(json.load(inf))}\n",
    "\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn, context_dim=None):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.norm_context = nn.LayerNorm(context_dim) if exists(context_dim) else None\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if exists(self.norm_context):\n",
    "            context = kwargs[\"context\"]\n",
    "            normed_context = self.norm_context(context)\n",
    "            kwargs.update(context=normed_context)\n",
    "\n",
    "        return self.fn(x, **kwargs)\n",
    "\n",
    "\n",
    "class GEGLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x, gates = x.chunk(2, dim=-1)\n",
    "        return x * F.gelu(gates)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, mult=4):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * mult * 2), GEGLU(), nn.Linear(dim * mult, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, query_dim, context_dim=None, heads=8, dim_head=64):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        context_dim = default(context_dim, query_dim)\n",
    "        self.scale = dim_head**-0.5\n",
    "        self.heads = heads\n",
    "\n",
    "        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)\n",
    "        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias=False)\n",
    "        self.to_out = nn.Linear(inner_dim, query_dim)\n",
    "\n",
    "    def forward(self, x, context=None, mask=None):\n",
    "        h = self.heads\n",
    "\n",
    "        q = self.to_q(x)\n",
    "        context = default(context, x)\n",
    "        k, v = self.to_kv(context).chunk(2, dim=-1)\n",
    "\n",
    "        q, k, v = map(lambda t: rearrange(t, \"b n (h d) -> (b h) n d\", h=h), (q, k, v))\n",
    "\n",
    "        sim = einsum(\"b i d, b j d -> b i j\", q, k) * self.scale\n",
    "\n",
    "        if exists(mask):\n",
    "            mask = rearrange(mask, \"b ... -> b (...)\")\n",
    "            max_neg_value = -torch.finfo(sim.dtype).max\n",
    "            mask = repeat(mask, \"b j -> (b h) () j\", h=h)\n",
    "            sim.masked_fill_(~mask, max_neg_value)\n",
    "\n",
    "        # attention, what we cannot get enough of\n",
    "        attn = sim.softmax(dim=-1)\n",
    "\n",
    "        out = einsum(\"b i j, b j d -> b i d\", attn, v)\n",
    "        out = rearrange(out, \"(b h) n d -> b n (h d)\", h=h)\n",
    "        return self.to_out(out)\n",
    "\n",
    "\n",
    "# Main classes #####################################################################################\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "class Encoder(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        mask_prob,\n",
    "        mask_ignore_token_ids,\n",
    "        mask_token_id,\n",
    "        emb_dim,\n",
    "        logits_dim,\n",
    "        depth,\n",
    "        num_latents,\n",
    "        latent_dim,\n",
    "        cross_heads,\n",
    "        latent_heads,\n",
    "        cross_dim_head,\n",
    "        latent_dim_head,\n",
    "        weight_tie_layers,\n",
    "        seq_dropout_prob,\n",
    "        nbins: int = DEFAULT_BINS,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.mask_ignore_token_ids = set(mask_ignore_token_ids)\n",
    "        self.mask_prob = mask_prob\n",
    "        # TODO: clean up later\n",
    "        self.mask_token_id = mask_token_id\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        self.model = PerceiverIO(\n",
    "            dim=emb_dim,  # dimension of sequence to be encoded\n",
    "            queries_dim=emb_dim,  # dimension of decoder queries\n",
    "            logits_dim=logits_dim,  # dimension of final logits\n",
    "            depth=depth,  # depth of net\n",
    "            num_latents=num_latents,  # number of latents, or induced set points, or centroids. different papers giving it different names\n",
    "            latent_dim=latent_dim,  # latent dimension\n",
    "            cross_heads=cross_heads,  # number of heads for cross attention. paper said 1\n",
    "            latent_heads=latent_heads,  # number of heads for latent self attention, 8\n",
    "            cross_dim_head=cross_dim_head,  # number of dimensions per cross attention head\n",
    "            latent_dim_head=latent_dim_head,  # number of dimensions per latent self attention head\n",
    "            weight_tie_layers=weight_tie_layers,  # whether to weight tie layers (optional, as indicated in the diagram)\n",
    "            seq_dropout_prob=seq_dropout_prob,  # fraction of the tokens from the input sequence to dropout (structured dropout, for saving compute and regularizing effects)\n",
    "        )\n",
    "\n",
    "        self.seq_dropout_prob = seq_dropout_prob\n",
    "\n",
    "        self.queries = torch.randn(MAX_LEN, self.emb_dim)  # latent_dim\n",
    "        self.emb = nn.Embedding(VOCAB_SIZE + nbins + 2, self.emb_dim)\n",
    "        self.pos_emb = nn.Embedding(MAX_LEN, self.emb_dim)  # +1 for exp\n",
    "\n",
    "    def forward(self, gid, bin_t, pad_mask):\n",
    "        x = self.emb(gid)\n",
    "        x += self.emb(bin_t)\n",
    "\n",
    "        # n, device = x.shape[1], x.device\n",
    "        # pos_emb = self.pos_emb(torch.arange(n, device=device))\n",
    "        # pos_emb = rearrange(pos_emb, \"n d -> () n d\")\n",
    "        # x = x + pos_emb\n",
    "\n",
    "        z = self.model(x, pad_mask)\n",
    "        return x, z\n",
    "\n",
    "\n",
    "class Decoder(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self, emb_dim, logits_dim, latent_dim, cross_heads, cross_dim_head, decoder_ff\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.decoder_cross_attn = PreNorm(\n",
    "            emb_dim,\n",
    "            Attention(emb_dim, latent_dim, heads=cross_heads, dim_head=cross_dim_head),\n",
    "            context_dim=latent_dim,\n",
    "        )\n",
    "        self.decoder_ff = PreNorm(emb_dim, FeedForward(emb_dim)) if decoder_ff else None\n",
    "        self.to_logits = (\n",
    "            nn.Linear(emb_dim, logits_dim) if exists(logits_dim) else nn.Identity()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, z):\n",
    "        latents = self.decoder_cross_attn(x, context=z)\n",
    "        latents = latents + self.decoder_ff(latents)\n",
    "        return self.to_logits(latents)\n",
    "\n",
    "\n",
    "class CellGP(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        lr=1e-4,\n",
    "        mask_prob=0.15,\n",
    "        mask_ignore_token_ids=[0],\n",
    "        mask_token_id=1,\n",
    "        pad_token_id=0,\n",
    "        emb_dim=256,\n",
    "        logits_dim_enc=None,\n",
    "        logits_dim_dec=1,\n",
    "        depth=6,\n",
    "        num_latents=256,\n",
    "        latent_dim=256,\n",
    "        cross_heads=1,\n",
    "        latent_heads=8,\n",
    "        cross_dim_head=64,\n",
    "        latent_dim_head=64,\n",
    "        weight_tie_layers=False,\n",
    "        seq_dropout_prob=0.1,\n",
    "        nbins: int = DEFAULT_BINS,\n",
    "        tokenizer=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.lr = lr\n",
    "        assert tokenizer is not None\n",
    "\n",
    "        self.mask_prob = mask_prob\n",
    "        self.mask_token_id = mask_token_id\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            mask_prob,\n",
    "            mask_ignore_token_ids,\n",
    "            mask_token_id,\n",
    "            emb_dim,\n",
    "            logits_dim_enc,\n",
    "            depth,\n",
    "            num_latents,\n",
    "            latent_dim,\n",
    "            cross_heads,\n",
    "            latent_heads,\n",
    "            cross_dim_head,\n",
    "            latent_dim_head,\n",
    "            weight_tie_layers,\n",
    "            seq_dropout_prob,\n",
    "            nbins=nbins,\n",
    "        )\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            emb_dim,\n",
    "            logits_dim=logits_dim_dec,\n",
    "            latent_dim=latent_dim,\n",
    "            cross_heads=cross_heads,\n",
    "            cross_dim_head=cross_dim_head,\n",
    "            decoder_ff=True,\n",
    "        )\n",
    "\n",
    "        # self.decoder = MLPDecoder(\n",
    "        #    emb_dim,\n",
    "        # )\n",
    "\n",
    "        for p in self.parameters():\n",
    "            p.requires_grad_()\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.mask_ignore_token_ids = set(mask_ignore_token_ids)\n",
    "\n",
    "    def forward(self, gid, bin_t, pad_mask):\n",
    "        x_emb, z = self.encoder(gid, bin_t, pad_mask)\n",
    "        bin_hat = self.decoder(x_emb, z)\n",
    "        return bin_hat\n",
    "\n",
    "    def _mask_with_tokens(self, t, token_ids):\n",
    "        init_no_mask = torch.full_like(t, False, dtype=torch.bool)\n",
    "        mask = reduce(lambda acc, el: acc | (t == el), token_ids, init_no_mask)\n",
    "        return mask\n",
    "\n",
    "    def _prob_mask_like(self, t, prob):\n",
    "        return torch.zeros_like(t).float().uniform_(0, 1) < prob\n",
    "\n",
    "    def _get_mask_subset_with_prob(self, t, mask, prob):\n",
    "        batch, seq_len, device = *mask.shape, mask.device\n",
    "        max_masked = math.ceil(prob * seq_len)\n",
    "\n",
    "        num_tokens = mask.sum(dim=-1, keepdim=True)\n",
    "        mask_excess = mask.cumsum(dim=-1) > (num_tokens * prob).ceil()\n",
    "        mask_excess = mask_excess[:, :max_masked]\n",
    "\n",
    "        rand = torch.rand((batch, seq_len), device=device).masked_fill(~mask, -1e9)\n",
    "        _, sampled_indices = rand.topk(max_masked, dim=-1)\n",
    "        sampled_indices = (sampled_indices + 1).masked_fill_(mask_excess, 0)\n",
    "\n",
    "        new_mask = torch.zeros((batch, seq_len + 1), device=device)\n",
    "        new_mask.scatter_(-1, sampled_indices, 1)\n",
    "        return new_mask[:, 1:].bool()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._shared_eval_step(batch, batch_idx)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self._shared_eval_step(batch, batch_idx)\n",
    "        self.log(\"val_loss\", loss, sync_dist=True)\n",
    "\n",
    "    def _shared_eval_step(self, batch, batch_idx):\n",
    "        gid_orig, bin_orig = batch\n",
    "        no_mask = self._mask_with_tokens(gid_orig, self.mask_ignore_token_ids)\n",
    "        pad_mask = ~no_mask\n",
    "        gid_max = gid_orig.max()\n",
    "\n",
    "        # if masked training\n",
    "        if self.mask_prob > 0:\n",
    "            mask = self._get_mask_subset_with_prob(gid_orig, pad_mask, self.mask_prob)\n",
    "            gid_t = gid_orig.masked_fill(mask, self.mask_token_id)\n",
    "            bin_t = bin_orig.masked_fill(mask, self.mask_token_id)\n",
    "\n",
    "        bin_hat = self(gid_t, bin_t, pad_mask)\n",
    "\n",
    "        bin_hat_batch = torch.cat(\n",
    "            [bin_hat[i, mask[i, :]] for i in range(bin_hat.shape[0])]\n",
    "        )\n",
    "        bin_orig_batch = torch.cat(\n",
    "            [bin_orig[i, mask[i, :]] - gid_max - 1 for i in range(bin_orig.shape[0])],\n",
    "        )\n",
    "        loss = self.criterion(bin_hat_batch, bin_orig_batch)\n",
    "        return loss\n",
    "\n",
    "\n",
    "def fixed_bin_collate(batch, vocab, genes, bins):\n",
    "    \"\"\"\n",
    "    Collate function for dataloader providing binning on a per cell basis\n",
    "    \"\"\"\n",
    "    assert bins > 0\n",
    "    bins = (\n",
    "        bins - 2\n",
    "    )  # searchsorted produces an extra bin on the left or right, and zero genes should get bin 0\n",
    "    extra_tokens = 2\n",
    "\n",
    "    # this array allows us to remap vocab integers to integers between 0-1\n",
    "    index = torch.full((max(vocab.values()) + 1,), -1, dtype=torch.int64)\n",
    "    index[genes] = torch.arange(0, len(genes), dtype=torch.int64)\n",
    "\n",
    "    gid_batch = torch.zeros((len(batch), len(genes)), dtype=torch.int64)\n",
    "    bins_batch = torch.full(\n",
    "        (len(batch), len(genes)), len(genes) + extra_tokens, dtype=torch.int64\n",
    "    )\n",
    "\n",
    "    for i, (gid, exp, med) in enumerate(batch):\n",
    "        if len(gid) == 0:\n",
    "            continue\n",
    "        try:\n",
    "            gid_t = torch.tensor(gid, dtype=torch.int64)\n",
    "            gid_batch[i] = index[genes] + extra_tokens\n",
    "            exp_t = torch.tensor(exp)\n",
    "            exp_t = exp_t[index[gid_t] != -1]\n",
    "            exp_t = torch.log1p(exp_t * med / exp_t.sum())\n",
    "            bin_e = torch.linspace(exp_t.min(), exp_t.max(), bins)\n",
    "            bins_batch[i, index[gid_t[index[gid_t] != -1]]] = (\n",
    "                torch.searchsorted(bin_e, exp_t, side=\"right\")\n",
    "                + len(genes)\n",
    "                + extra_tokens\n",
    "                + 1\n",
    "            )\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    return gid_batch, bins_batch\n",
    "\n",
    "\n",
    "class SCDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.hf = h5.File(path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf[\"gid\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.hf[\"gid\"][idx], self.hf[\"exp\"][idx], self.hf[\"med\"][idx]\n",
    "\n",
    "\n",
    "class DM(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        path,\n",
    "        vocab,\n",
    "        subset_genes,\n",
    "        nbins,\n",
    "        train_percentage=0.85,\n",
    "        val_percentage=0.10,\n",
    "        test_percentage=0.05,\n",
    "        batch_size=16,\n",
    "        num_workers=16,\n",
    "        timeout=5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.path = path\n",
    "\n",
    "        self.nbins = nbins\n",
    "        self.vocab = vocab\n",
    "        self.collate_fn = partial(\n",
    "            fixed_bin_collate,\n",
    "            vocab=self.vocab,\n",
    "            genes=subset_genes,\n",
    "            bins=self.nbins,\n",
    "        )\n",
    "        self.train_percentage = train_percentage\n",
    "        self.val_percentage = val_percentage\n",
    "        self.test_percentage = test_percentage\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.timeout = timeout\n",
    "\n",
    "    def setup(self, stage: str = \"default\"):\n",
    "        self.dset = SCDataset(self.path)\n",
    "        self.dset_train, self.dset_val, self.dset_test = random_split(\n",
    "            self.dset,\n",
    "            [self.train_percentage, self.val_percentage, self.test_percentage],\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.dset_train,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            timeout=self.timeout,\n",
    "            collate_fn=self.collate_fn,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        \"\"\"\n",
    "        Returns a DataLoader that loads the validation dataset.\n",
    "        \"\"\"\n",
    "        return DataLoader(\n",
    "            self.dset_val,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            timeout=self.timeout,\n",
    "            collate_fn=self.collate_fn,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        \"\"\"\n",
    "        Returns a DataLoader that loads the holdout (test) dataset.\n",
    "        \"\"\"\n",
    "        return DataLoader(\n",
    "            self.dset_test,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            timeout=self.timeout,\n",
    "            collate_fn=self.collate_fn,\n",
    "        )\n",
    "\n",
    "    def teardown(self, stage: str):\n",
    "        pass\n",
    "\n",
    "\n",
    "def test_collate():\n",
    "    vocab = {v: k for k, v in enumerate(\"abcdefghij\")}\n",
    "    genes = [0, 3, 9, 2, 4]\n",
    "    bins = 3\n",
    "    print(vocab)\n",
    "    collate_fn = partial(\n",
    "        fixed_bin_collate,\n",
    "        vocab=vocab,\n",
    "        genes=genes,\n",
    "        bins=bins,\n",
    "    )\n",
    "    gid = [3, 1, 2, 7, 5]\n",
    "    exp = [1, 2, 3, 4, 5]\n",
    "    med = 5\n",
    "    batch = [[gid, exp, med]]\n",
    "    print(collate_fn(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = read_vocab(\"../vocab_ccle.json\")\n",
    "VOCAB_SIZE = len(tokenizer)\n",
    "BINS = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ccle_vocab_genes.json\", \"r\") as inf:\n",
    "    ccle_vocab_genes = sorted(json.load(inf))\n",
    "MAX_LEN = len(ccle_vocab_genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_collate()\n",
    "\n",
    "dm = DM(\n",
    "    \"/storage/ujp/processed_raw.h5\",\n",
    "    vocab=tokenizer,\n",
    "    subset_genes=ccle_vocab_genes,\n",
    "    nbins=BINS,\n",
    "    train_percentage=0.85,\n",
    "    val_percentage=0.10,\n",
    "    test_percentage=0.05,\n",
    "    batch_size=16,\n",
    "    num_workers=16,\n",
    "    timeout=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12587195,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_h5_file = h5.File(\"/storage/ujp/processed_raw.h5\")\n",
    "raw_h5_file['exp'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_file_path = \"/storage/ujp/processed_raw.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (12587195,)\n",
      "Subset file created with 10000 cells at /Mounts/rbg-storage1/users/johnyang/cellot/datasets/process_raw_10k_subset.h5\n"
     ]
    }
   ],
   "source": [
    "# Path to the new HDF5 file\n",
    "subset_file_path = '/Mounts/rbg-storage1/users/johnyang/cellot/datasets/process_raw_10k_subset.h5'\n",
    "\n",
    "# Number of cells to include in the subset\n",
    "num_cells = 10000\n",
    "\n",
    "# Open the original HDF5 file\n",
    "with h5.File(original_file_path, 'r') as original_file:\n",
    "    # Read the 'exp' dataset\n",
    "    original_exp = original_file['exp']\n",
    "    \n",
    "    # Check the shape of the 'exp' dataset\n",
    "    original_shape = original_exp.shape\n",
    "    print(f\"Original shape: {original_shape}\")\n",
    "\n",
    "    # Check if the original dataset has at least num_cells cells\n",
    "    if original_shape[0] < num_cells:\n",
    "        print(f\"The original dataset has fewer than {num_cells} cells.\")\n",
    "    else:\n",
    "        # Create the new HDF5 file\n",
    "        with h5.File(subset_file_path, 'w') as subset_file:\n",
    "            # Create a new dataset in the new file with the same name ('exp')\n",
    "            # but only the first num_cells cells\n",
    "            subset_file.create_dataset('exp', data=original_exp[:num_cells])\n",
    "\n",
    "        print(f\"Subset file created with {num_cells} cells at {subset_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjohnyang\u001b[0m (\u001b[33mprotein-optimization\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20230807_162847-6163wgag</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/protein-optimization/CellGP_Encoder/runs/6163wgag' target=\"_blank\">rare-frost-2</a></strong> to <a href='https://wandb.ai/protein-optimization/CellGP_Encoder' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/protein-optimization/CellGP_Encoder' target=\"_blank\">https://wandb.ai/protein-optimization/CellGP_Encoder</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/protein-optimization/CellGP_Encoder/runs/6163wgag' target=\"_blank\">https://wandb.ai/protein-optimization/CellGP_Encoder/runs/6163wgag</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/rsg/chemistry/johnyang/miniconda3/envs/cot2/lib/python3.9/site-packages/lightning_fabric/connector.py:554: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/7\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/7\n",
      "Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/7\n",
      "Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/7\n",
      "Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/7\n",
      "Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/7\n",
      "Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/7\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 7 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "/data/rsg/chemistry/johnyang/miniconda3/envs/cot2/lib/python3.9/site-packages/torch/cuda/__init__.py:145: UserWarning: \n",
      "NVIDIA A100-PCIE-40GB with CUDA capability sm_80 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.\n",
      "If you want to use the NVIDIA A100-PCIE-40GB GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n",
      "/data/rsg/chemistry/johnyang/miniconda3/envs/cot2/lib/python3.9/site-packages/torch/cuda/__init__.py:145: UserWarning: \n",
      "NVIDIA A100-PCIE-40GB with CUDA capability sm_80 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.\n",
      "If you want to use the NVIDIA A100-PCIE-40GB GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n",
      "/data/rsg/chemistry/johnyang/miniconda3/envs/cot2/lib/python3.9/site-packages/torch/cuda/__init__.py:145: UserWarning: \n",
      "NVIDIA A100-PCIE-40GB with CUDA capability sm_80 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.\n",
      "If you want to use the NVIDIA A100-PCIE-40GB GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n",
      "/data/rsg/chemistry/johnyang/miniconda3/envs/cot2/lib/python3.9/site-packages/torch/cuda/__init__.py:145: UserWarning: \n",
      "NVIDIA A100-PCIE-40GB with CUDA capability sm_80 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.\n",
      "If you want to use the NVIDIA A100-PCIE-40GB GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n",
      "/data/rsg/chemistry/johnyang/miniconda3/envs/cot2/lib/python3.9/site-packages/torch/cuda/__init__.py:145: UserWarning: \n",
      "NVIDIA A100-PCIE-40GB with CUDA capability sm_80 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.\n",
      "If you want to use the NVIDIA A100-PCIE-40GB GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n",
      "/data/rsg/chemistry/johnyang/miniconda3/envs/cot2/lib/python3.9/site-packages/torch/cuda/__init__.py:145: UserWarning: \n",
      "NVIDIA A100-PCIE-40GB with CUDA capability sm_80 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.\n",
      "If you want to use the NVIDIA A100-PCIE-40GB GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n",
      "/data/rsg/chemistry/johnyang/miniconda3/envs/cot2/lib/python3.9/site-packages/torch/cuda/__init__.py:145: UserWarning: \n",
      "NVIDIA A100-PCIE-40GB with CUDA capability sm_80 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.\n",
      "If you want to use the NVIDIA A100-PCIE-40GB GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n"
     ]
    },
    {
     "ename": "ProcessRaisedException",
     "evalue": "\n\n-- Process 0 terminated with the following error:\nTraceback (most recent call last):\n  File \"/data/rsg/chemistry/johnyang/miniconda3/envs/cot2/lib/python3.9/site-packages/torch/multiprocessing/spawn.py\", line 69, in _wrap\n    fn(i, *args)\n  File \"/data/rsg/chemistry/johnyang/miniconda3/envs/cot2/lib/python3.9/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py\", line 147, in _wrapping_function\n    results = function(*args, **kwargs)\n  File \"/data/rsg/chemistry/johnyang/miniconda3/envs/cot2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 568, in _fit_impl\n    self._run(model, ckpt_path=ckpt_path)\n  File \"/data/rsg/chemistry/johnyang/miniconda3/envs/cot2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 932, in _run\n    self.__setup_profiler()\n  File \"/data/rsg/chemistry/johnyang/miniconda3/envs/cot2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1062, in __setup_profiler\n    self.profiler.setup(stage=self.state.fn, local_rank=local_rank, log_dir=self.log_dir)\n  File \"/data/rsg/chemistry/johnyang/miniconda3/envs/cot2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1182, in log_dir\n    dirpath = self.strategy.broadcast(dirpath)\n  File \"/data/rsg/chemistry/johnyang/miniconda3/envs/cot2/lib/python3.9/site-packages/pytorch_lightning/strategies/ddp.py\", line 291, in broadcast\n    torch.distributed.broadcast_object_list(obj, src, group=_group.WORLD)\n  File \"/data/rsg/chemistry/johnyang/miniconda3/envs/cot2/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py\", line 1869, in broadcast_object_list\n    broadcast(object_sizes_tensor, src=src, group=group)\n  File \"/data/rsg/chemistry/johnyang/miniconda3/envs/cot2/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py\", line 1187, in broadcast\n    work = default_pg.broadcast([tensor], opts)\nRuntimeError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:47, unhandled cuda error, NCCL version 21.0.3\nncclUnhandledCudaError: Call to CUDA function failed.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProcessRaisedException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 39\u001b[0m\n\u001b[1;32m     29\u001b[0m cback \u001b[39m=\u001b[39m ModelCheckpoint(monitor\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m\"\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m\"\u001b[39m, save_top_k\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     30\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(\n\u001b[1;32m     31\u001b[0m     accelerator\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpu\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     32\u001b[0m     strategy\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     callbacks\u001b[39m=\u001b[39m[cback],\n\u001b[1;32m     38\u001b[0m )\n\u001b[0;32m---> 39\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model, dm)\n",
      "File \u001b[0;32m/data/rsg/chemistry/johnyang/miniconda3/envs/cot2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:529\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    527\u001b[0m model \u001b[39m=\u001b[39m _maybe_unwrap_optimized(model)\n\u001b[1;32m    528\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 529\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    530\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    531\u001b[0m )\n",
      "File \u001b[0;32m/data/rsg/chemistry/johnyang/miniconda3/envs/cot2/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:41\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 41\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39;49mstrategy\u001b[39m.\u001b[39;49mlauncher\u001b[39m.\u001b[39;49mlaunch(trainer_fn, \u001b[39m*\u001b[39;49margs, trainer\u001b[39m=\u001b[39;49mtrainer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     42\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     44\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m/data/rsg/chemistry/johnyang/miniconda3/envs/cot2/lib/python3.9/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py:124\u001b[0m, in \u001b[0;36m_MultiProcessingLauncher.launch\u001b[0;34m(self, function, trainer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m process_context \u001b[39m=\u001b[39m mp\u001b[39m.\u001b[39mstart_processes(\n\u001b[1;32m    117\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrapping_function,\n\u001b[1;32m    118\u001b[0m     args\u001b[39m=\u001b[39mprocess_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    121\u001b[0m     join\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,  \u001b[39m# we will join ourselves to get the process references\u001b[39;00m\n\u001b[1;32m    122\u001b[0m )\n\u001b[1;32m    123\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocs \u001b[39m=\u001b[39m process_context\u001b[39m.\u001b[39mprocesses\n\u001b[0;32m--> 124\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m process_context\u001b[39m.\u001b[39;49mjoin():\n\u001b[1;32m    125\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    127\u001b[0m worker_output \u001b[39m=\u001b[39m return_queue\u001b[39m.\u001b[39mget()\n",
      "File \u001b[0;32m/data/rsg/chemistry/johnyang/miniconda3/envs/cot2/lib/python3.9/site-packages/torch/multiprocessing/spawn.py:160\u001b[0m, in \u001b[0;36mProcessContext.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    158\u001b[0m msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-- Process \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m terminated with the following error:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m error_index\n\u001b[1;32m    159\u001b[0m msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m original_trace\n\u001b[0;32m--> 160\u001b[0m \u001b[39mraise\u001b[39;00m ProcessRaisedException(msg, error_index, failed_process\u001b[39m.\u001b[39mpid)\n",
      "\u001b[0;31mProcessRaisedException\u001b[0m: \n\n-- Process 0 terminated with the following error:\nTraceback (most recent call last):\n  File \"/data/rsg/chemistry/johnyang/miniconda3/envs/cot2/lib/python3.9/site-packages/torch/multiprocessing/spawn.py\", line 69, in _wrap\n    fn(i, *args)\n  File \"/data/rsg/chemistry/johnyang/miniconda3/envs/cot2/lib/python3.9/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py\", line 147, in _wrapping_function\n    results = function(*args, **kwargs)\n  File \"/data/rsg/chemistry/johnyang/miniconda3/envs/cot2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 568, in _fit_impl\n    self._run(model, ckpt_path=ckpt_path)\n  File \"/data/rsg/chemistry/johnyang/miniconda3/envs/cot2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 932, in _run\n    self.__setup_profiler()\n  File \"/data/rsg/chemistry/johnyang/miniconda3/envs/cot2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1062, in __setup_profiler\n    self.profiler.setup(stage=self.state.fn, local_rank=local_rank, log_dir=self.log_dir)\n  File \"/data/rsg/chemistry/johnyang/miniconda3/envs/cot2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1182, in log_dir\n    dirpath = self.strategy.broadcast(dirpath)\n  File \"/data/rsg/chemistry/johnyang/miniconda3/envs/cot2/lib/python3.9/site-packages/pytorch_lightning/strategies/ddp.py\", line 291, in broadcast\n    torch.distributed.broadcast_object_list(obj, src, group=_group.WORLD)\n  File \"/data/rsg/chemistry/johnyang/miniconda3/envs/cot2/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py\", line 1869, in broadcast_object_list\n    broadcast(object_sizes_tensor, src=src, group=group)\n  File \"/data/rsg/chemistry/johnyang/miniconda3/envs/cot2/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py\", line 1187, in broadcast\n    work = default_pg.broadcast([tensor], opts)\nRuntimeError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:47, unhandled cuda error, NCCL version 21.0.3\nncclUnhandledCudaError: Call to CUDA function failed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = CellGP(\n",
    "    lr=1e-4,\n",
    "    mask_prob=0.5,\n",
    "    mask_ignore_token_ids=[0],\n",
    "    mask_token_id=1,\n",
    "    emb_dim=256,\n",
    "    logits_dim_enc=None,\n",
    "    logits_dim_dec=BINS,\n",
    "    depth=8,\n",
    "    num_latents=256,\n",
    "    latent_dim=256,\n",
    "    cross_heads=1,\n",
    "    latent_heads=8,\n",
    "    cross_dim_head=256,\n",
    "    latent_dim_head=256,\n",
    "    weight_tie_layers=False,\n",
    "    seq_dropout_prob=0.1,\n",
    "    nbins=BINS,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# trainer = pl.Trainer(\n",
    "#     accelerator=\"cpu\",\n",
    "#     precision=32,\n",
    "#     max_epochs=1,\n",
    "# )\n",
    "\n",
    "wandb_logger = WandbLogger(log_model=\"all\", project=\"CellGP_Encoder\")\n",
    "cback = ModelCheckpoint(monitor=\"val_loss\", mode=\"min\", save_top_k=1)\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    strategy=\"auto\",\n",
    "    devices=7,\n",
    "    precision=16,\n",
    "    max_epochs=11,\n",
    "    logger=wandb_logger,\n",
    "    callbacks=[cback],\n",
    ")\n",
    "trainer.fit(model, dm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cot2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
