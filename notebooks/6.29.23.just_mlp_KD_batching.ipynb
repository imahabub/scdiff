{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from absl import logging\n",
    "from absl.flags import FLAGS\n",
    "from cellot import losses\n",
    "from cellot.utils.loaders import load\n",
    "from cellot.models.cellot import compute_loss_f, compute_loss_g, compute_w2_distance\n",
    "from cellot.train.summary import Logger\n",
    "from cellot.data.utils import cast_loader_to_iterator\n",
    "from cellot.models.ae import compute_scgen_shift\n",
    "from tqdm import trange\n",
    "\n",
    "from cellot.models.ae import AutoEncoder\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')\n",
    "logger = logging.getLogger(\"data_logger\")\n",
    "logger.setLevel(logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True\n",
    "TARGET = 'all' if not DEBUG else 'abexinostat'\n",
    "LATENT_DIM = 50\n",
    "\n",
    "from pathlib import Path\n",
    "outdir_path = '/Mounts/rbg-storage1/users/johnyang/cellot/results/sciplex3/full_ae'\n",
    "outdir = Path(outdir_path)\n",
    "\n",
    "# %%\n",
    "outdir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "cachedir = outdir / \"cache\"\n",
    "cachedir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Using GPUs: 5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import GPUtil\n",
    "import os\n",
    "\n",
    "def get_free_gpu():\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(device)\n",
    "    # Set environment variables for which GPUs to use.\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    chosen_gpu = ''.join(\n",
    "        [str(x) for x in GPUtil.getAvailable(order='memory')])\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = chosen_gpu\n",
    "    print(f\"Using GPUs: {chosen_gpu}\")\n",
    "    return chosen_gpu\n",
    "\n",
    "status = cachedir / \"status\"\n",
    "status.write_text(\"running\")\n",
    "\n",
    "device = f'cuda:{get_free_gpu()}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "import omegaconf\n",
    "\n",
    "if DEBUG:\n",
    "    n_iters = 250000\n",
    "    batch_size = 256\n",
    "else:\n",
    "    n_iters = 250000\n",
    "    batch_size = 256\n",
    "\n",
    "yaml_str = f\"\"\"\n",
    "model:\n",
    "   name: scgen\n",
    "   beta: 0.0\n",
    "   dropout: 0.0\n",
    "   hidden_units: [512, 512]\n",
    "   latent_dim: 50\n",
    "\n",
    "optim:\n",
    "   lr: 0.001\n",
    "   optimizer: Adam\n",
    "   weight_decay: 1.0e-05\n",
    "\n",
    "scheduler:\n",
    "   gamma: 0.5\n",
    "   step_size: 100000\n",
    "\n",
    "training:\n",
    "  cache_freq: 10000\n",
    "  eval_freq: 2500\n",
    "  logs_freq: 250\n",
    "  n_iters: {n_iters}\n",
    "\n",
    "data:\n",
    "  type: cell\n",
    "  source: control\n",
    "  condition: drug\n",
    "  path: /Mounts/rbg-storage1/users/johnyang/cellot/datasets/scrna-sciplex3/hvg.h5ad\n",
    "  target: {TARGET}\n",
    "\n",
    "datasplit:\n",
    "    groupby: drug   \n",
    "    name: train_test\n",
    "    test_size: 0.2\n",
    "    random_state: 0\n",
    "\n",
    "dataloader:\n",
    "    batch_size: {batch_size}\n",
    "    shuffle: true\n",
    "\"\"\"\n",
    "\n",
    "config = omegaconf.OmegaConf.create(yaml_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ### Utils\n",
    "\n",
    "# %%\n",
    "def load_lr_scheduler(optim, config):\n",
    "    if \"scheduler\" not in config:\n",
    "        return None\n",
    "\n",
    "    return torch.optim.lr_scheduler.StepLR(optim, **config.scheduler)\n",
    "\n",
    "def check_loss(*args):\n",
    "    for arg in args:\n",
    "        if torch.isnan(arg):\n",
    "            raise ValueError\n",
    "\n",
    "\n",
    "def load_item_from_save(path, key, default):\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        return default\n",
    "\n",
    "    ckpt = torch.load(path)\n",
    "    if key not in ckpt:\n",
    "        logging.warn(f\"'{key}' not found in ckpt: {str(path)}\")\n",
    "        return default\n",
    "\n",
    "    return ckpt[key]\n",
    "\n",
    "# %%\n",
    "import cellot.models\n",
    "from cellot.data.cell import load_cell_data\n",
    "\n",
    "\n",
    "def load_data(config, **kwargs):\n",
    "    data_type = config.get(\"data.type\", \"cell\")\n",
    "    if data_type in [\"cell\", \"cell-merged\", \"tupro-cohort\"]:\n",
    "        loadfxn = load_cell_data\n",
    "\n",
    "    elif data_type == \"toy\":\n",
    "        loadfxn = load_toy_data\n",
    "\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    return loadfxn(config, **kwargs)\n",
    "\n",
    "\n",
    "def load_model(config, device, restore=None, **kwargs):\n",
    "    # def load_autoencoder_model(config, restore=None, **kwargs):\n",
    "    \n",
    "    def load_optimizer(config, params):\n",
    "        kwargs = dict(config.get(\"optim\", {}))\n",
    "        assert kwargs.pop(\"optimizer\", \"Adam\") == \"Adam\"\n",
    "        optim = torch.optim.Adam(params, **kwargs)\n",
    "        return optim\n",
    "\n",
    "\n",
    "    def load_networks(config, **kwargs):\n",
    "        kwargs = kwargs.copy()\n",
    "        kwargs.update(dict(config.get(\"model\", {})))\n",
    "        name = kwargs.pop(\"name\")\n",
    "\n",
    "        if name == \"scgen\":\n",
    "            model = AutoEncoder\n",
    "\n",
    "        # elif name == \"cae\":\n",
    "        #     model = ConditionalAutoEncoder\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        return model(**kwargs)\n",
    "    \n",
    "    model = load_networks(config, **kwargs)\n",
    "    optim = load_optimizer(config, model.parameters())\n",
    "\n",
    "    if restore is not None and Path(restore).exists():\n",
    "        print('Loading model from checkpoint')\n",
    "        ckpt = torch.load(restore, map_location=device)\n",
    "        model.load_state_dict(ckpt[\"model_state\"])\n",
    "        optim.load_state_dict(ckpt[\"optim_state\"])\n",
    "        if config.model.name == \"scgen\" and \"code_means\" in ckpt:\n",
    "            model.code_means = ckpt[\"code_means\"]\n",
    "            \n",
    "    # logger.info(f'Model on device {next(model.parameters()).device}')\n",
    "\n",
    "    return model, optim\n",
    "\n",
    "def load(config, device, restore=None, include_model_kwargs=False, **kwargs):\n",
    "\n",
    "    loader, model_kwargs = load_data(config, include_model_kwargs=True, **kwargs)\n",
    "\n",
    "    model, opt = load_model(config, device, restore=restore, **model_kwargs)\n",
    "\n",
    "    # if include_model_kwargs:\n",
    "    #     return model, opt, loader, model_kwargs\n",
    "\n",
    "    return model, opt, loader\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_dict(model, optim, **kwargs):\n",
    "    state = {\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optim_state\": optim.state_dict(),\n",
    "    }\n",
    "\n",
    "    if hasattr(model, \"code_means\"):\n",
    "        state[\"code_means\"] = model.code_means\n",
    "\n",
    "    state.update(kwargs)\n",
    "\n",
    "    return state\n",
    "\n",
    "def evaluate(vinputs):\n",
    "    with torch.no_grad():\n",
    "        loss, comps, _ = model(vinputs)\n",
    "        loss = loss.mean()\n",
    "        comps = {k: v.mean().item() for k, v in comps._asdict().items()}\n",
    "        check_loss(loss)\n",
    "        logger.log(\"eval\", loss=loss.item(), step=step, **comps)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _, _, loader = load(config, 'cuda', restore=cachedir / \"last.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"R^3 diffusion methods.\"\"\"\n",
    "import numpy as np\n",
    "from scipy.special import gamma\n",
    "import torch\n",
    "\n",
    "\n",
    "class R3Diffuser:\n",
    "    \"\"\"VP-SDE diffuser class for translations.\"\"\"\n",
    "\n",
    "    def __init__(self, r3_conf):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            min_b: starting value in variance schedule.\n",
    "            max_b: ending value in variance schedule.\n",
    "        \"\"\"\n",
    "        self._r3_conf = r3_conf\n",
    "        self.min_b = r3_conf.min_b\n",
    "        self.max_b = r3_conf.max_b\n",
    "        self.schedule = r3_conf.schedule\n",
    "        self._score_scaling = r3_conf.score_scaling\n",
    "        self.latent_dim = r3_conf.latent_dim\n",
    "\n",
    "    def _scale(self, x):\n",
    "        return x * self._r3_conf.coordinate_scaling\n",
    "\n",
    "    def _unscale(self, x):\n",
    "        return x / self._r3_conf.coordinate_scaling\n",
    "\n",
    "    def b_t(self, t):\n",
    "        if np.any(t < 0) or np.any(t > 1):\n",
    "            raise ValueError(f'Invalid t={t}')\n",
    "        if self.schedule == 'linear': \n",
    "            return self.min_b + t*(self.max_b - self.min_b)\n",
    "        elif self.schedule == 'cosine':\n",
    "            return self.max_b + 0.5*(self.min_b - self.max_b)*(1 + np.cos(t*np.pi))\n",
    "        elif self.schedule == 'exponential':\n",
    "            sigma = t * np.log10(self.max_b) + (1 - t) * np.log10(self.min_b)\n",
    "            return 10 ** sigma\n",
    "        else:\n",
    "            raise ValueError(f'Unknown schedule {self.schedule}')\n",
    "    \n",
    "    def diffusion_coef(self, t):\n",
    "        \"\"\"Time-dependent diffusion coefficient.\"\"\"\n",
    "        return np.sqrt(self.b_t(t))\n",
    "\n",
    "    def drift_coef(self, x, t):\n",
    "        \"\"\"Time-dependent drift coefficient.\"\"\"\n",
    "        return -1/2 * self.b_t(t) * x\n",
    "\n",
    "    def sample_ref(self, n_samples: float=1):\n",
    "        return np.random.normal(size=(n_samples, self.latent_dim))\n",
    "\n",
    "    def marginal_b_t(self, t):\n",
    "        if self.schedule == 'linear':\n",
    "            return t*self.min_b + (1/2)*(t**2)*(self.max_b-self.min_b)\n",
    "        elif self.schedule == 'exponential': \n",
    "            return (self.max_b**t * self.min_b**(1-t) - self.min_b) / (\n",
    "                np.log(self.max_b) - np.log(self.min_b))\n",
    "        else:\n",
    "            raise ValueError(f'Unknown schedule {self.schedule}')\n",
    "\n",
    "    def calc_trans_0(self, score_t, x_t, t, use_torch=True):\n",
    "        beta_t = self.marginal_b_t(t)\n",
    "        beta_t = beta_t[..., None, None]\n",
    "        exp_fn = torch.exp if use_torch else np.exp\n",
    "        cond_var = 1 - exp_fn(-beta_t)\n",
    "        return (score_t * cond_var + x_t) / exp_fn(-1/2*beta_t)\n",
    "\n",
    "    def forward(self, x_t_1: np.ndarray, t: float, num_t: int):\n",
    "        \"\"\"Samples marginal p(x(t) | x(t-1)).\n",
    "\n",
    "        Args:\n",
    "            x_0: [..., n, 3] initial positions in Angstroms.\n",
    "            t: continuous time in [0, 1]. \n",
    "\n",
    "        Returns:\n",
    "            x_t: [..., n, 3] positions at time t in Angstroms.\n",
    "            score_t: [..., n, 3] score at time t in scaled Angstroms.\n",
    "        \"\"\"\n",
    "        if not np.isscalar(t):\n",
    "            raise ValueError(f'{t} must be a scalar.')\n",
    "        x_t_1 = self._scale(x_t_1)\n",
    "        b_t = torch.tensor(self.marginal_b_t(t) / num_t).to(x_t_1.device)\n",
    "        z_t_1 = torch.tensor(np.random.normal(size=x_t_1.shape)).to(x_t_1.device)\n",
    "        x_t = torch.sqrt(1 - b_t) * x_t_1 + torch.sqrt(b_t) * z_t_1\n",
    "        return x_t\n",
    "    \n",
    "    def distribution(self, x_t, score_t, t, mask, dt):\n",
    "        x_t = self._scale(x_t)\n",
    "        g_t = self.diffusion_coef(t)\n",
    "        f_t = self.drift_coef(x_t, t)\n",
    "        std = g_t * np.sqrt(dt)\n",
    "        mu = x_t - (f_t - g_t**2 * score_t) * dt\n",
    "        if mask is not None:\n",
    "            mu *= mask[..., None]\n",
    "        return mu, std\n",
    "\n",
    "    def forward_marginal(self, x_0: np.ndarray, t: float):\n",
    "        \"\"\"Samples marginal p(x(t) | x(0)).\n",
    "\n",
    "        Args:\n",
    "            x_0: [..., n, 3] initial positions in Angstroms.\n",
    "            t: continuous time in [0, 1]. \n",
    "\n",
    "        Returns:\n",
    "            x_t: [..., n, 3] positions at time t in Angstroms.\n",
    "            score_t: [..., n, 3] score at time t in scaled Angstroms.\n",
    "        \"\"\"\n",
    "        if not np.isscalar(t):\n",
    "            raise ValueError(f'{t} must be a scalar.')\n",
    "        x_0 = self._scale(x_0)\n",
    "        x_t = np.random.normal(\n",
    "            loc=np.exp(-1/2*self.marginal_b_t(t)) * x_0,\n",
    "            scale=np.sqrt(1 - np.exp(-self.marginal_b_t(t)))\n",
    "        )\n",
    "        score_t = self.score(x_t, x_0, t)\n",
    "        x_t = self._unscale(x_t)\n",
    "        return x_t, score_t\n",
    "\n",
    "    def score_scaling(self, t: float):\n",
    "        if self._score_scaling == 'var':\n",
    "            return 1 / self.conditional_var(t)\n",
    "        elif self._score_scaling == 'std':\n",
    "            return 1 / np.sqrt(self.conditional_var(t))\n",
    "        elif self._score_scaling == 'expected_norm':\n",
    "            return np.sqrt(2) / (gamma(1.5) * np.sqrt(self.conditional_var(t)))\n",
    "        else:\n",
    "            raise ValueError(f'Unrecognized scaling {self._score_scaling}')\n",
    "\n",
    "    def reverse(\n",
    "            self,\n",
    "            *,\n",
    "            x_t: np.ndarray,\n",
    "            score_t: np.ndarray,\n",
    "            t: float,\n",
    "            dt: float,\n",
    "            mask: np.ndarray=None,\n",
    "            center: bool=True,\n",
    "            ode: bool=False,\n",
    "            noise_scale: float=1.0,\n",
    "        ):\n",
    "        \"\"\"Simulates the reverse SDE for 1 step\n",
    "\n",
    "        Args:\n",
    "            x_t: [..., 3] current positions at time t in angstroms.\n",
    "            score_t: [..., 3] rotation score at time t.\n",
    "            t: continuous time in [0, 1].\n",
    "            dt: continuous step size in [0, 1].\n",
    "            mask: True indicates which residues to diffuse.\n",
    "\n",
    "        Returns:\n",
    "            [..., 3] positions at next step t-1.\n",
    "        \"\"\"\n",
    "        if not np.isscalar(t):\n",
    "            raise ValueError(f'{t} must be a scalar.')\n",
    "        x_t = self._scale(x_t)\n",
    "        g_t = self.diffusion_coef(t)\n",
    "        f_t = self.drift_coef(x_t, t)\n",
    "        if ode:\n",
    "            # Probability flow ODE\n",
    "            perturb = (f_t - (1/2)*(g_t**2) * score_t) * dt\n",
    "        else:\n",
    "            # Usual stochastic dynamics\n",
    "            z = noise_scale * np.random.normal(size=score_t.shape)\n",
    "            perturb = (f_t - g_t**2 * score_t) * dt + g_t * np.sqrt(dt) * z\n",
    "\n",
    "        if mask is not None:\n",
    "            perturb *= mask[..., None]\n",
    "        else:\n",
    "            mask = np.ones(x_t.shape[:-1])\n",
    "        x_t_1 = x_t - perturb\n",
    "        if center:\n",
    "            com = np.sum(x_t_1, axis=-2) / np.sum(mask, axis=-1)[..., None]\n",
    "            x_t_1 -= com[..., None, :]\n",
    "        x_t_1 = self._unscale(x_t_1)\n",
    "        return x_t_1\n",
    "\n",
    "    def conditional_var(self, t, use_torch=False):\n",
    "        \"\"\"Conditional variance of p(xt|x0).\n",
    "\n",
    "        Var[x_t|x_0] = conditional_var(t)*I\n",
    "\n",
    "        \"\"\"\n",
    "        if use_torch:\n",
    "            return 1 - torch.exp(-self.marginal_b_t(t))\n",
    "        return 1 - np.exp(-self.marginal_b_t(t))\n",
    "\n",
    "    def score(self, x_t, x_0, t, use_torch=False, scale=False):\n",
    "        if use_torch:\n",
    "            exp_fn = torch.exp\n",
    "        else:\n",
    "            exp_fn = np.exp\n",
    "        if scale:\n",
    "            x_t = self._scale(x_t)\n",
    "            x_0 = self._scale(x_0)\n",
    "        return -(x_t - exp_fn(-1/2*self.marginal_b_t(t)) * x_0) / self.conditional_var(t, use_torch=use_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "r3_conf = OmegaConf.create({\n",
    "    'min_b': 0.01,\n",
    "    'max_b': 1.0,\n",
    "    'schedule': 'linear',\n",
    "    'score_scaling': 'var',\n",
    "    'coordinate_scaling': 1.0,\n",
    "    'latent_dim': LATENT_DIM,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffuser = R3Diffuser(r3_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer, Linear\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dim = 64\n",
    "num_layers = 2\n",
    "nhead = 1\n",
    "dim_feedforward = 128\n",
    "dropout = 0.1 if not DEBUG else 0.0\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import functools as fn\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, input_dim=128, hidden_dim=64, output_dim=50):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "def get_timestep_embedding(timesteps, embedding_dim, max_positions=10000):\n",
    "    # Code from https://github.com/hojonathanho/diffusion/blob/master/diffusion_tf/nn.py\n",
    "    assert len(timesteps.shape) == 1\n",
    "    timesteps = timesteps * max_positions\n",
    "    half_dim = embedding_dim // 2\n",
    "    emb = math.log(max_positions) / (half_dim - 1)\n",
    "    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32, device=timesteps.device) * -emb)\n",
    "    emb = timesteps.float()[:, None] * emb[None, :]\n",
    "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
    "    if embedding_dim % 2 == 1:  # zero pad\n",
    "        emb = F.pad(emb, (0, 1), mode='constant')\n",
    "    assert emb.shape == (timesteps.shape[0], embedding_dim)\n",
    "    return emb\n",
    "\n",
    "class ScoreNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScoreNetwork, self).__init__()\n",
    "        \n",
    "        self.latent_dim = LATENT_DIM\n",
    "        self.model_dim = model_dim\n",
    "        self.dropout = dropout\n",
    "        print(f'Dropout is {self.dropout}')\n",
    "        self.embed_code_and_t = nn.Linear(LATENT_DIM + model_dim, model_dim)\n",
    "        # self.trmr_layer = TransformerEncoderLayer(d_model=model_dim, nhead=8, dim_feedforward=2048, dropout=dropout)\n",
    "        self.pred_score = FeedForward(input_dim=model_dim, hidden_dim=64, output_dim=LATENT_DIM)\n",
    "        self.model = nn.ModuleList([self.embed_code_and_t, self.pred_score]) #*[self.trmr_layer for _ in range(num_layers)], self.pred_score])\n",
    "        \n",
    "        self.timestep_embedder = fn.partial(\n",
    "            get_timestep_embedding,\n",
    "            embedding_dim=self.model_dim,\n",
    "            # max_positions=100\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        device = x.device\n",
    "        B, C = x.shape\n",
    "        t_embed = torch.tile(self.timestep_embedder(torch.tensor([t]).to(device)), dims=[B, 1])\n",
    "        \n",
    "        x = torch.cat([x, t_embed], dim=-1).to(device)\n",
    "        \n",
    "        for module in self.model[:-1]:  # iterate over all modules except the last one\n",
    "            x = module(x)\n",
    "        x = self.model[-1](x.squeeze(0))  # pass through the last module (FeedForward)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropout is 0.0\n"
     ]
    }
   ],
   "source": [
    "score_network = ScoreNetwork().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14770"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in score_network.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(score_network.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                         | 0/250000 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "STEP = 0\n",
    "ticker = trange(STEP, n_iters, initial=STEP, total=n_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterator = cast_loader_to_iterator(loader, cycle_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = next(iterator.train).to(device)\n",
    "# inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ae.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     ex_code = ae.encode(ex_batch).to(device)[None, 0, :1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ex_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 50])\n",
      "tensor([[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [  1.,   1.,   1.,  ...,   1.,   1.,   1.],\n",
      "        [  2.,   2.,   2.,  ...,   2.,   2.,   2.],\n",
      "        ...,\n",
      "        [253., 253., 253.,  ..., 253., 253., 253.],\n",
      "        [254., 254., 254.,  ..., 254., 254., 254.],\n",
      "        [255., 255., 255.,  ..., 255., 255., 255.]], device='cuda:5')\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor of values from 0 to 255\n",
    "values = torch.arange(256).float().to(device)\n",
    "\n",
    "# Add an extra dimension to this tensor and replicate it along the new dimension\n",
    "ex_code = values.unsqueeze(1).expand(-1, 50)\n",
    "\n",
    "print(ex_code.shape)  # Outputs: torch.Size([256, 50])\n",
    "print(ex_code)  # Outputs a 256x50 tensor where each row i contains the value i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 50])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_code.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_t = 0.0\n",
    "rng = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('runs/6.29.23_just_mlp_KD_batching')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(dt=0.001):\n",
    "    score_network.eval()\n",
    "    # log_freq = (1 / dt) / 100\n",
    "    ex_code_np = ex_code.detach().cpu().numpy()\n",
    "    with torch.no_grad():\n",
    "        x_t, _ = diffuser.forward_marginal(ex_code_np, t=1.0)\n",
    "        \n",
    "        for i, t in enumerate(np.arange(1.0, 0, -dt)):\n",
    "            # if i % log_freq == 0:\n",
    "                # print(x_t)\n",
    "            x_t = torch.tensor(x_t).float().to(device)\n",
    "            pred_score = score_network(x_t, t)\n",
    "            \n",
    "            # pred_scores.append(pred_score)\n",
    "            # gt_scores.append(gt_score)\n",
    "            \n",
    "            # _, gt_score = diffuser.forward_marginal(ex_code_np.detach().cpu().numpy(), t=t)\n",
    "\n",
    "            # print(pred_score, gt_score)\n",
    "            \n",
    "            x_t = diffuser.reverse(x_t=x_t.detach().cpu().numpy(), score_t=pred_score.detach().cpu().numpy(), t=t, dt=dt, center=False)\n",
    "        \n",
    "        x_0 = x_t\n",
    "        writer.add_embedding(x_0, global_step=step, tag='reverse_sampled_x_0')\n",
    "        writer.add_embedding(ex_code_np, global_step=step, tag='gt_x_0')\n",
    "        euclidean_distance = np.linalg.norm(x_0 - ex_code_np)\n",
    "        writer.add_scalar('sampled_gt_dist', euclidean_distance, global_step=step)\n",
    "        \n",
    "        return x_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At step 0, TRAINING loss is 872.6554551147299\n",
      "At step 0, sampled x_0 is [[ 1.02796395  0.51237074]\n",
      " [-0.13835956  2.03075597]]\n",
      "At step 250, TRAINING loss is 99.81891947133073\n",
      "At step 500, TRAINING loss is 4869.650151452273\n",
      "At step 750, TRAINING loss is 3407.3039797228657\n",
      "At step 1000, TRAINING loss is 2255.2377913351957\n",
      "At step 1000, sampled x_0 is [[-0.29557414 -0.60666118]\n",
      " [-0.29559674  2.84456002]]\n",
      "At step 1250, TRAINING loss is 2271.250339482431\n",
      "At step 1500, TRAINING loss is 3342.055818482766\n",
      "At step 1750, TRAINING loss is 4975.948728008503\n",
      "At step 2000, TRAINING loss is 221.68205110310987\n",
      "At step 2000, sampled x_0 is [[0.22096445 0.67141534]\n",
      " [1.19126193 0.75107358]]\n",
      "At step 2250, TRAINING loss is 1431.894845561876\n",
      "At step 2500, TRAINING loss is 1488.664210488067\n",
      "At step 2750, TRAINING loss is 652.5694562655735\n",
      "At step 3000, TRAINING loss is 2884.551349078899\n",
      "At step 3000, sampled x_0 is [[ 0.1554642  -1.03517112]\n",
      " [ 0.29189294  0.04878044]]\n",
      "At step 3250, TRAINING loss is 2124.134117103801\n",
      "At step 3500, TRAINING loss is 2240.113486250538\n",
      "At step 3750, TRAINING loss is 2702.378666399231\n",
      "At step 4000, TRAINING loss is 3336.3293838519457\n",
      "At step 4000, sampled x_0 is [[-0.77064407 -0.30460122]\n",
      " [-0.93214284 -2.24837723]]\n",
      "At step 4250, TRAINING loss is 2512.4880132837525\n",
      "At step 4500, TRAINING loss is 2610.8694071364985\n",
      "At step 4750, TRAINING loss is 490.36630424452994\n",
      "At step 5000, TRAINING loss is 3226.76700999852\n",
      "At step 5000, sampled x_0 is [[-0.9245525  -2.51070577]\n",
      " [ 2.98772583  2.47561581]]\n",
      "At step 5250, TRAINING loss is 146.49645403440888\n",
      "At step 5500, TRAINING loss is 552.2515489663851\n",
      "At step 5750, TRAINING loss is 2395.8670496526593\n",
      "At step 6000, TRAINING loss is 463.7360482443901\n",
      "At step 6000, sampled x_0 is [[-0.0851462  -0.05505699]\n",
      " [ 2.04591679  0.64326396]]\n",
      "At step 6250, TRAINING loss is 1620.722429912866\n",
      "At step 6500, TRAINING loss is 1612.8165358587285\n",
      "At step 6750, TRAINING loss is 56.74132181263767\n",
      "At step 7000, TRAINING loss is 2648.4187499098543\n",
      "At step 7000, sampled x_0 is [[-0.08684761 -2.54490919]\n",
      " [-0.08340666  2.60697163]]\n",
      "At step 7250, TRAINING loss is 4162.820274783943\n",
      "At step 7500, TRAINING loss is 2599.4462546986356\n",
      "At step 7750, TRAINING loss is 115.93238191664429\n",
      "At step 8000, TRAINING loss is 4686.172065248902\n",
      "At step 8000, sampled x_0 is [[ 0.92065979 -1.66042165]\n",
      " [ 0.18426074  1.44379578]]\n",
      "At step 8250, TRAINING loss is 1472.9044623940963\n",
      "At step 8500, TRAINING loss is 2422.8595462183803\n",
      "At step 8750, TRAINING loss is 3945.239671072047\n",
      "At step 9000, TRAINING loss is 1551.697272815576\n",
      "At step 9000, sampled x_0 is [[ 0.76217025  0.3101668 ]\n",
      " [-1.08062083  1.31939478]]\n",
      "At step 9250, TRAINING loss is 2330.2644728611212\n",
      "At step 9500, TRAINING loss is 2404.6334232925883\n",
      "At step 9750, TRAINING loss is 30.331728960195182\n",
      "At step 10000, TRAINING loss is 690.6859702455004\n",
      "At step 10000, sampled x_0 is [[-3.02616122  0.1330862 ]\n",
      " [ 1.40103601  0.01245719]]\n",
      "At step 10250, TRAINING loss is 260.60227199708936\n",
      "At step 10500, TRAINING loss is 4107.471439149178\n",
      "At step 10750, TRAINING loss is 609.9602976615114\n",
      "At step 11000, TRAINING loss is 86.57195435800759\n",
      "At step 11000, sampled x_0 is [[ 0.69912868 -0.0076779 ]\n",
      " [-1.64272497 -0.39043997]]\n",
      "At step 11250, TRAINING loss is 4211.638489942014\n",
      "At step 11500, TRAINING loss is 362.34590134671845\n",
      "At step 11750, TRAINING loss is 3678.971986705907\n",
      "At step 12000, TRAINING loss is 3039.1487088791223\n",
      "At step 12000, sampled x_0 is [[ 0.14396786  0.28646696]\n",
      " [ 1.40435975 -0.66871927]]\n",
      "At step 12250, TRAINING loss is 203.66637236817508\n",
      "At step 12500, TRAINING loss is 116.33002689498592\n",
      "At step 12750, TRAINING loss is 395.0597904856487\n",
      "At step 15000, TRAINING loss is 5022.08124987754\n",
      "At step 15000, sampled x_0 is [[2.32873394 0.96814728]\n",
      " [0.18137702 0.0591761 ]]\n",
      "At step 15250, TRAINING loss is 2605.788095714275\n",
      "At step 15500, TRAINING loss is 1621.3721616622279\n",
      "At step 15750, TRAINING loss is 1060.32609449177\n",
      "At step 16000, TRAINING loss is 3471.17304241281\n",
      "At step 16000, sampled x_0 is [[ 1.18784722  0.08889295]\n",
      " [ 0.72654526 -0.3537281 ]]\n",
      "At step 16250, TRAINING loss is 4115.680058823416\n",
      "At step 16500, TRAINING loss is 104.42191120328518\n",
      "At step 16750, TRAINING loss is 62.094970881727136\n",
      "At step 17000, TRAINING loss is 35.9901255292577\n",
      "At step 17000, sampled x_0 is [[ 0.06631156 -1.13830817]\n",
      " [ 1.20288514 -0.58316462]]\n",
      "At step 17250, TRAINING loss is 287.1529166959243\n",
      "At step 17500, TRAINING loss is 420.26219072162786\n",
      "At step 17750, TRAINING loss is 1.6572681886135303\n",
      "At step 18000, TRAINING loss is 2728.3584749105557\n",
      "At step 18000, sampled x_0 is [[1.07828433 1.97283685]\n",
      " [3.00140774 1.97377629]]\n",
      "At step 18250, TRAINING loss is 18.458502283271145\n",
      "At step 18500, TRAINING loss is 1056.884271340526\n",
      "At step 18750, TRAINING loss is 4480.629682381459\n",
      "At step 19000, TRAINING loss is 2409.7450250424304\n",
      "At step 19000, sampled x_0 is [[-2.64239412 -0.44771435]\n",
      " [ 0.75940683 -0.56252015]]\n",
      "At step 19250, TRAINING loss is 2233.2127207641056\n",
      "At step 19500, TRAINING loss is 1504.120559862004\n",
      "At step 19750, TRAINING loss is 4127.092257114475\n",
      "At step 20000, TRAINING loss is 797.080231118988\n",
      "At step 20000, sampled x_0 is [[ 1.68764611 -0.15461319]\n",
      " [ 2.15805493 -0.43525516]]\n",
      "At step 20250, TRAINING loss is 4215.0355739902925\n",
      "At step 20500, TRAINING loss is 568.5005162183822\n",
      "At step 20750, TRAINING loss is 99.94585090149158\n",
      "At step 21000, TRAINING loss is 757.753987672291\n",
      "At step 21000, sampled x_0 is [[ 1.11424424 -0.7677366 ]\n",
      " [ 0.10371742  0.40447769]]\n",
      "At step 21250, TRAINING loss is 3018.2810582005345\n",
      "At step 21500, TRAINING loss is 1811.7671521647612\n",
      "At step 21750, TRAINING loss is 829.5921712491187\n",
      "At step 22000, TRAINING loss is 15.28986341057779\n",
      "At step 22000, sampled x_0 is [[-1.34117917  0.40920486]\n",
      " [ 0.57227442 -0.17101304]]\n",
      "At step 22250, TRAINING loss is 410.6536810876282\n",
      "At step 22500, TRAINING loss is 1194.6755216887964\n",
      "At step 22750, TRAINING loss is 3991.5913582691564\n",
      "At step 23000, TRAINING loss is 2844.0441765701103\n",
      "At step 23000, sampled x_0 is [[-0.26406016  2.35216673]\n",
      " [ 0.90439074  1.51162122]]\n",
      "At step 23250, TRAINING loss is 3452.326673796181\n",
      "At step 23500, TRAINING loss is 4158.081888759626\n",
      "At step 23750, TRAINING loss is 3861.876538297587\n",
      "At step 24000, TRAINING loss is 3755.8583689982424\n",
      "At step 24000, sampled x_0 is [[-0.03769596 -0.80229381]\n",
      " [ 0.77030382  2.48581806]]\n",
      "At step 24250, TRAINING loss is 3422.0337991296324\n",
      "At step 24500, TRAINING loss is 36.02837634404496\n",
      "At step 24750, TRAINING loss is 3737.1060915428807\n",
      "At step 25000, TRAINING loss is 1804.0280680859275\n",
      "At step 25000, sampled x_0 is [[ 0.47291671  0.40884822]\n",
      " [-0.47881542  0.71470741]]\n",
      "At step 25250, TRAINING loss is 222.38736318830217\n",
      "At step 25500, TRAINING loss is 4068.66004544688\n",
      "At step 25750, TRAINING loss is 2289.3314832937103\n",
      "At step 26000, TRAINING loss is 4804.698146242535\n",
      "At step 26000, sampled x_0 is [[0.26340253 0.43642571]\n",
      " [2.46091728 0.64113133]]\n",
      "At step 26250, TRAINING loss is 690.4471588376491\n",
      "At step 26500, TRAINING loss is 202.8342853040049\n",
      "At step 26750, TRAINING loss is 2345.9923271787907\n",
      "At step 27000, TRAINING loss is 566.4188025358994\n",
      "At step 27000, sampled x_0 is [[-2.03734289  0.69702346]\n",
      " [ 2.72402926  0.74531123]]\n",
      "At step 27250, TRAINING loss is 4477.770719854253\n",
      "At step 27500, TRAINING loss is 128.1346309418302\n",
      "At step 27750, TRAINING loss is 2333.981926310919\n",
      "At step 28000, TRAINING loss is 8.943069606520192\n",
      "At step 28000, sampled x_0 is [[-1.3646569  -1.14293844]\n",
      " [ 0.20109249  0.8818308 ]]\n",
      "At step 28250, TRAINING loss is 300.85178305113266\n",
      "At step 28500, TRAINING loss is 901.3084371886457\n",
      "At step 28750, TRAINING loss is 1036.1898703687393\n",
      "At step 29000, TRAINING loss is 4067.6210083016585\n",
      "At step 29000, sampled x_0 is [[ 0.38345864 -1.27722945]\n",
      " [ 2.35955735  2.20441471]]\n",
      "At step 29250, TRAINING loss is 962.8016953397654\n",
      "At step 29500, TRAINING loss is 759.0292763631046\n",
      "At step 29750, TRAINING loss is 2167.575743035729\n",
      "At step 30000, TRAINING loss is 109.38780610068899\n",
      "At step 30000, sampled x_0 is [[-0.85211563  1.50395335]\n",
      " [ 0.06710848  0.02047002]]\n",
      "At step 30250, TRAINING loss is 1147.8500727575133\n",
      "At step 30500, TRAINING loss is 4419.8684862832715\n",
      "At step 30750, TRAINING loss is 1151.7908095858597\n",
      "At step 31000, TRAINING loss is 4521.926611070478\n",
      "At step 31000, sampled x_0 is [[-1.2960295   0.68762239]\n",
      " [ 1.91123292  0.97667671]]\n",
      "At step 31250, TRAINING loss is 152.7282574736842\n",
      "At step 31500, TRAINING loss is 801.3977189164013\n",
      "At step 31750, TRAINING loss is 900.8990720284603\n",
      "At step 32000, TRAINING loss is 70.40673380521359\n",
      "At step 32000, sampled x_0 is [[-0.00588529  0.46472055]\n",
      " [ 2.5214427   0.29255295]]\n",
      "At step 32250, TRAINING loss is 1856.8822542735293\n",
      "At step 32500, TRAINING loss is 66.40879553477102\n",
      "At step 32750, TRAINING loss is 2673.7094219443943\n",
      "At step 33000, TRAINING loss is 2456.736912532913\n",
      "At step 33000, sampled x_0 is [[-0.21245185 -0.76611881]\n",
      " [ 1.46013321 -0.92514669]]\n",
      "At step 33250, TRAINING loss is 15.06450738512774\n",
      "At step 33500, TRAINING loss is 2654.892417715589\n",
      "At step 33750, TRAINING loss is 10.836335525012736\n",
      "At step 34000, TRAINING loss is 201.10163658806573\n",
      "At step 34000, sampled x_0 is [[0.4591991  1.63381496]\n",
      " [2.73932111 2.33516164]]\n",
      "At step 34250, TRAINING loss is 3760.894385855891\n",
      "At step 34500, TRAINING loss is 3139.595314538514\n",
      "At step 34750, TRAINING loss is 19.034715386048433\n",
      "At step 35000, TRAINING loss is 48.932040764501416\n",
      "At step 35000, sampled x_0 is [[-0.47987835  0.27163584]\n",
      " [ 0.37169772  1.31233338]]\n",
      "At step 35250, TRAINING loss is 923.9940053319833\n",
      "At step 35500, TRAINING loss is 860.6299264518962\n",
      "At step 35750, TRAINING loss is 1627.2644879514658\n",
      "At step 36000, TRAINING loss is 1731.0099905324403\n",
      "At step 36000, sampled x_0 is [[0.73959781 0.43460863]\n",
      " [0.87962131 0.78100478]]\n",
      "At step 36250, TRAINING loss is 4473.146989793153\n",
      "At step 36500, TRAINING loss is 304.54904015395846\n",
      "At step 36750, TRAINING loss is 1886.2812797821084\n",
      "At step 37000, TRAINING loss is 936.0992290848822\n",
      "At step 37000, sampled x_0 is [[-0.73178364 -1.5297042 ]\n",
      " [ 2.36249319 -0.25606382]]\n",
      "At step 37250, TRAINING loss is 3181.828250282747\n",
      "At step 37500, TRAINING loss is 344.7810064652372\n",
      "At step 37750, TRAINING loss is 391.4445867256656\n",
      "At step 38000, TRAINING loss is 1083.1760819780661\n",
      "At step 38000, sampled x_0 is [[1.19899048 0.49198692]\n",
      " [1.51895917 0.25298625]]\n",
      "At step 38250, TRAINING loss is 3177.2518430667023\n",
      "At step 38500, TRAINING loss is 3374.8449909749706\n",
      "At step 38750, TRAINING loss is 3015.818603280887\n",
      "At step 39000, TRAINING loss is 2475.7585297557316\n",
      "At step 39000, sampled x_0 is [[-3.14273249 -0.67453211]\n",
      " [ 2.00185359  1.74112323]]\n",
      "At step 39250, TRAINING loss is 3927.0903131028135\n",
      "At step 39500, TRAINING loss is 2783.5331642004417\n",
      "At step 39750, TRAINING loss is 1973.478173515749\n",
      "At step 40000, TRAINING loss is 1211.0053974077919\n",
      "At step 40000, sampled x_0 is [[-2.18853106 -0.50806782]\n",
      " [-0.38518311  0.75532181]]\n",
      "At step 40250, TRAINING loss is 1774.6858868534555\n",
      "At step 43250, TRAINING loss is 266.5902725392968\n",
      "At step 43500, TRAINING loss is 3065.397165723469\n",
      "At step 43750, TRAINING loss is 2645.3369208252857\n",
      "At step 44000, TRAINING loss is 1.6555914466479686\n",
      "At step 44000, sampled x_0 is [[-2.77802363  0.3439174 ]\n",
      " [ 1.79354196 -0.01762402]]\n",
      "At step 44250, TRAINING loss is 536.3809867100005\n",
      "At step 44500, TRAINING loss is 246.27081550665403\n",
      "At step 44750, TRAINING loss is 868.4026562587861\n",
      "At step 45000, TRAINING loss is 552.0029296650487\n",
      "At step 45000, sampled x_0 is [[-0.59959011 -0.31161031]\n",
      " [ 1.24243878  0.81213583]]\n",
      "At step 45250, TRAINING loss is 2194.658380800697\n",
      "At step 45500, TRAINING loss is 336.1903626180105\n",
      "At step 45750, TRAINING loss is 3351.0506791754874\n",
      "At step 46000, TRAINING loss is 1211.0877886480914\n",
      "At step 46000, sampled x_0 is [[0.32710634 1.15268667]\n",
      " [0.18875345 2.18884101]]\n",
      "At step 46250, TRAINING loss is 1646.8487952474618\n",
      "At step 46500, TRAINING loss is 4473.60859352431\n",
      "At step 46750, TRAINING loss is 923.1111262905158\n",
      "At step 47000, TRAINING loss is 897.0071321136127\n",
      "At step 47000, sampled x_0 is [[-0.06330638 -0.02015542]\n",
      " [ 2.72691777  1.07993668]]\n",
      "At step 47250, TRAINING loss is 908.0352038013207\n",
      "At step 47500, TRAINING loss is 49.237205079484625\n",
      "At step 47750, TRAINING loss is 167.1844311526567\n",
      "At step 48000, TRAINING loss is 4480.601388747129\n",
      "At step 48000, sampled x_0 is [[ 0.71217118 -0.57790084]\n",
      " [ 0.58616111  1.05993632]]\n",
      "At step 48250, TRAINING loss is 4739.279143416621\n",
      "At step 48500, TRAINING loss is 3034.8951439774755\n",
      "At step 48750, TRAINING loss is 3269.2009071136217\n",
      "At step 49000, TRAINING loss is 678.7134804067413\n",
      "At step 49000, sampled x_0 is [[ 0.12425606 -0.14807454]\n",
      " [-0.34286957  1.55374629]]\n",
      "At step 49250, TRAINING loss is 2578.0843269878533\n",
      "At step 49500, TRAINING loss is 428.9977584707565\n",
      "At step 49750, TRAINING loss is 952.9895048933176\n",
      "At step 50000, TRAINING loss is 3218.8124974979883\n",
      "At step 50000, sampled x_0 is [[-2.20251703 -0.51995326]\n",
      " [ 1.45602278  0.38020819]]\n",
      "At step 50250, TRAINING loss is 148.11855907779562\n",
      "At step 50500, TRAINING loss is 2933.767564465801\n",
      "At step 50750, TRAINING loss is 1391.6888990613238\n",
      "At step 51000, TRAINING loss is 1207.7232493430688\n",
      "At step 51000, sampled x_0 is [[0.33965096 0.03115207]\n",
      " [0.8552805  1.2258487 ]]\n",
      "At step 51250, TRAINING loss is 1539.0575541631197\n",
      "At step 51500, TRAINING loss is 116.26913809803057\n",
      "At step 51750, TRAINING loss is 1026.429022334174\n",
      "At step 52000, TRAINING loss is 1678.8788955290056\n",
      "At step 52000, sampled x_0 is [[-1.42014376  0.14325435]\n",
      " [-0.82022347 -0.51855085]]\n",
      "At step 52250, TRAINING loss is 2797.279373224564\n",
      "At step 52500, TRAINING loss is 329.0809805582501\n",
      "At step 52750, TRAINING loss is 0.6724924966744985\n",
      "At step 53000, TRAINING loss is 1484.2347737296425\n",
      "At step 53000, sampled x_0 is [[-1.13779534 -0.34311954]\n",
      " [ 2.90560405  0.3041835 ]]\n",
      "At step 53250, TRAINING loss is 1759.5083357705942\n",
      "At step 53500, TRAINING loss is 1449.053646428294\n",
      "At step 53750, TRAINING loss is 4485.460401834254\n",
      "At step 54000, TRAINING loss is 4814.855316792837\n",
      "At step 54000, sampled x_0 is [[-3.16447337 -1.27523552]\n",
      " [ 1.27714679  0.53030809]]\n",
      "At step 54250, TRAINING loss is 65.4316777214311\n",
      "At step 54500, TRAINING loss is 3108.812464036743\n",
      "At step 54750, TRAINING loss is 921.78073678198\n",
      "At step 55000, TRAINING loss is 1054.4696398785527\n",
      "At step 55000, sampled x_0 is [[-0.53616218  1.94010639]\n",
      " [-1.72985996  2.75980487]]\n",
      "At step 55250, TRAINING loss is 4595.831626734922\n",
      "At step 55500, TRAINING loss is 1010.444783142706\n",
      "At step 55750, TRAINING loss is 4208.868772004199\n",
      "At step 56000, TRAINING loss is 2725.3333593835223\n",
      "At step 56000, sampled x_0 is [[ 0.23067499 -0.3200718 ]\n",
      " [-0.61564054  1.31626568]]\n",
      "At step 56250, TRAINING loss is 3836.932001778015\n",
      "At step 56500, TRAINING loss is 7.240173105540676\n",
      "At step 56750, TRAINING loss is 3501.326929631241\n",
      "At step 57000, TRAINING loss is 1262.457891974618\n",
      "At step 57000, sampled x_0 is [[ 0.77009941 -0.84357353]\n",
      " [ 1.56356016  0.80847003]]\n",
      "At step 57250, TRAINING loss is 817.6374310459573\n",
      "At step 57500, TRAINING loss is 896.7471730003938\n",
      "At step 57750, TRAINING loss is 2224.6200398172255\n",
      "At step 58000, TRAINING loss is 468.2805108819314\n",
      "At step 58000, sampled x_0 is [[0.89571805 1.32871073]\n",
      " [0.10014619 0.80165727]]\n",
      "At step 58250, TRAINING loss is 3690.9584562420155\n",
      "At step 58500, TRAINING loss is 4112.537398844386\n",
      "At step 58750, TRAINING loss is 1773.3495780425526\n",
      "At step 59000, TRAINING loss is 4426.865558590296\n",
      "At step 59000, sampled x_0 is [[-0.51894547  1.29081897]\n",
      " [ 1.02061735  0.53644758]]\n",
      "At step 59250, TRAINING loss is 5.735840042734605\n",
      "At step 59500, TRAINING loss is 235.96757118137236\n",
      "At step 59750, TRAINING loss is 2729.762186452817\n",
      "At step 60000, TRAINING loss is 2520.110228145769\n",
      "At step 60000, sampled x_0 is [[0.70097323 0.21716371]\n",
      " [0.60027275 1.56995683]]\n",
      "At step 60250, TRAINING loss is 2551.375388984728\n",
      "At step 60500, TRAINING loss is 494.7726504827772\n",
      "At step 60750, TRAINING loss is 3702.243815511154\n",
      "At step 61000, TRAINING loss is 162.20521568665583\n",
      "At step 61000, sampled x_0 is [[-1.49044636  1.65004321]\n",
      " [-0.34558373  1.14915719]]\n",
      "At step 61250, TRAINING loss is 477.88150699654307\n",
      "At step 61500, TRAINING loss is 36.455188408227855\n",
      "At step 61750, TRAINING loss is 1204.4437753543498\n",
      "At step 62000, TRAINING loss is 526.8676634573733\n",
      "At step 62000, sampled x_0 is [[-0.14929507 -0.36635638]\n",
      " [ 2.54100083  0.95577543]]\n",
      "At step 62250, TRAINING loss is 4307.333106683664\n",
      "At step 62500, TRAINING loss is 840.3902971043849\n",
      "At step 62750, TRAINING loss is 2199.1348361216087\n",
      "At step 63000, TRAINING loss is 3693.721239906339\n",
      "At step 63000, sampled x_0 is [[1.18885916 1.6215848 ]\n",
      " [1.45922393 2.50569776]]\n",
      "At step 63250, TRAINING loss is 127.77916468384348\n",
      "At step 63500, TRAINING loss is 1496.6269946839775\n",
      "At step 63750, TRAINING loss is 3651.336983547247\n",
      "At step 64000, TRAINING loss is 2723.2474372183174\n",
      "At step 64000, sampled x_0 is [[-0.47767696 -0.08444729]\n",
      " [-0.35235917 -0.66832153]]\n",
      "At step 64250, TRAINING loss is 757.9927237167221\n",
      "At step 64500, TRAINING loss is 1520.198255049295\n",
      "At step 64750, TRAINING loss is 1928.1622406855072\n",
      "At step 65000, TRAINING loss is 701.917424135049\n",
      "At step 65000, sampled x_0 is [[ 0.54054353 -0.80525202]\n",
      " [ 0.61444355  0.49924279]]\n",
      "At step 65250, TRAINING loss is 39.732546361342315\n",
      "At step 65500, TRAINING loss is 2545.8851587506915\n",
      "At step 65750, TRAINING loss is 278.1433254737733\n",
      "At step 66000, TRAINING loss is 1250.7563101190792\n",
      "At step 66000, sampled x_0 is [[-1.6029187   2.75584637]\n",
      " [ 0.19774727  0.97411082]]\n",
      "At step 66250, TRAINING loss is 3591.0862267213365\n",
      "At step 66500, TRAINING loss is 0.8816629205996306\n",
      "At step 66750, TRAINING loss is 1832.8834902106623\n",
      "At step 67000, TRAINING loss is 154.13449236098853\n",
      "At step 67000, sampled x_0 is [[-1.42455571 -1.77589999]\n",
      " [-0.1981959   1.9808053 ]]\n",
      "At step 67250, TRAINING loss is 1902.8903908241725\n",
      "At step 67500, TRAINING loss is 847.7061994413391\n",
      "At step 67750, TRAINING loss is 4281.788795437935\n",
      "At step 68000, TRAINING loss is 998.5860353839375\n",
      "At step 68000, sampled x_0 is [[-1.17851242 -1.26930739]\n",
      " [ 0.06097196  0.49636728]]\n",
      "At step 68250, TRAINING loss is 2610.837234113697\n",
      "At step 68500, TRAINING loss is 2524.7230461099575\n",
      "At step 68750, TRAINING loss is 14.466788962961996\n",
      "At step 69000, TRAINING loss is 1080.845125685583\n",
      "At step 69000, sampled x_0 is [[-0.95120434 -0.62744687]\n",
      " [ 0.48451158  1.33586974]]\n",
      "At step 69250, TRAINING loss is 2079.8635248677447\n",
      "At step 69500, TRAINING loss is 102.97661285571169\n",
      "At step 69750, TRAINING loss is 3283.5092458639256\n",
      "At step 70000, TRAINING loss is 2257.2659087204897\n",
      "At step 70000, sampled x_0 is [[-1.79655769 -1.2182003 ]\n",
      " [-0.35067062 -0.49041037]]\n",
      "At step 70250, TRAINING loss is 468.9795362854587\n",
      "At step 70500, TRAINING loss is 940.2580865713571\n",
      "At step 70750, TRAINING loss is 2399.356704057195\n",
      "At step 71000, TRAINING loss is 80.93638054537084\n",
      "At step 71000, sampled x_0 is [[0.96681025 1.64521532]\n",
      " [2.62442053 1.07238366]]\n",
      "At step 71250, TRAINING loss is 3054.592363572854\n",
      "At step 71500, TRAINING loss is 515.6714130207251\n",
      "At step 71750, TRAINING loss is 3887.186559254807\n",
      "At step 72000, TRAINING loss is 729.8434064081325\n",
      "At step 72000, sampled x_0 is [[-0.02562256  0.2706647 ]\n",
      " [ 2.33413868  0.31653847]]\n",
      "At step 72250, TRAINING loss is 3273.3056001693344\n",
      "At step 72500, TRAINING loss is 55.72803845250613\n",
      "At step 72750, TRAINING loss is 2302.457248132766\n",
      "At step 73000, TRAINING loss is 4018.31341422085\n",
      "At step 73000, sampled x_0 is [[-0.10054031  0.78230062]\n",
      " [ 2.66711035  1.8840441 ]]\n",
      "At step 73250, TRAINING loss is 425.90911289705315\n",
      "At step 73500, TRAINING loss is 4475.270591289715\n",
      "At step 73750, TRAINING loss is 58.40338443611343\n",
      "At step 74000, TRAINING loss is 4032.1438704003526\n",
      "At step 74000, sampled x_0 is [[-0.10863072 -1.15518539]\n",
      " [ 1.97855133  0.79842544]]\n",
      "At step 74250, TRAINING loss is 101.57967919371683\n",
      "At step 74500, TRAINING loss is 742.5535667808924\n",
      "At step 74750, TRAINING loss is 62.76304119142847\n",
      "At step 75000, TRAINING loss is 4310.700070898168\n",
      "At step 75000, sampled x_0 is [[-1.48401498 -0.63263542]\n",
      " [ 1.0590705   0.10404523]]\n",
      "At step 75250, TRAINING loss is 3686.6700161132508\n",
      "At step 75500, TRAINING loss is 979.7028417030147\n",
      "At step 75750, TRAINING loss is 1499.4486623383495\n",
      "At step 76000, TRAINING loss is 461.16835070866006\n",
      "At step 76000, sampled x_0 is [[-0.4304252  -2.21958717]\n",
      " [-0.52945478  1.05414197]]\n",
      "At step 76250, TRAINING loss is 1975.6396913439485\n",
      "At step 76500, TRAINING loss is 973.1470145182677\n",
      "At step 76750, TRAINING loss is 887.9272450361316\n",
      "At step 77000, TRAINING loss is 4502.4765538724005\n",
      "At step 77000, sampled x_0 is [[-0.04565262  1.16930375]\n",
      " [ 2.03373726  2.00924094]]\n",
      "At step 77250, TRAINING loss is 3811.7857305147636\n",
      "At step 77500, TRAINING loss is 231.48363040494542\n",
      "At step 77750, TRAINING loss is 3868.452067859723\n",
      "At step 78000, TRAINING loss is 444.04273116592833\n",
      "At step 78000, sampled x_0 is [[-1.07605357  0.53707607]\n",
      " [ 1.7446578  -0.23816067]]\n",
      "At step 78250, TRAINING loss is 717.582698938268\n",
      "At step 78500, TRAINING loss is 4223.339925417491\n",
      "At step 78750, TRAINING loss is 212.95058154712902\n",
      "At step 79000, TRAINING loss is 242.20178056087192\n",
      "At step 79000, sampled x_0 is [[-0.7489194  -0.43535332]\n",
      " [ 1.09292263  0.5712394 ]]\n",
      "At step 79250, TRAINING loss is 29.85750709812953\n",
      "At step 79500, TRAINING loss is 4102.025593399421\n",
      "At step 79750, TRAINING loss is 162.83541653407428\n",
      "At step 80000, TRAINING loss is 41.265065177661356\n",
      "At step 80000, sampled x_0 is [[-0.63837997 -1.52955766]\n",
      " [ 2.72819342  0.77711672]]\n",
      "At step 80250, TRAINING loss is 1710.6799958702193\n",
      "At step 80500, TRAINING loss is 1802.817570287787\n",
      "At step 80750, TRAINING loss is 76.51096806225763\n",
      "At step 81000, TRAINING loss is 51.54654253751488\n",
      "At step 81000, sampled x_0 is [[-0.09886965  0.43139534]\n",
      " [ 2.5665551   0.45845722]]\n",
      "At step 81250, TRAINING loss is 850.9327241615362\n",
      "At step 81500, TRAINING loss is 240.86735929078418\n",
      "At step 81750, TRAINING loss is 1537.5707553579225\n",
      "At step 82000, TRAINING loss is 4605.44451838838\n",
      "At step 82000, sampled x_0 is [[-0.81285127 -1.01307546]\n",
      " [ 1.33088499  1.07910836]]\n",
      "At step 82250, TRAINING loss is 1487.5565158383806\n",
      "At step 82500, TRAINING loss is 0.48635457993359255\n",
      "At step 82750, TRAINING loss is 744.243683995234\n",
      "At step 83000, TRAINING loss is 142.31883461772748\n",
      "At step 83000, sampled x_0 is [[-0.80894505  1.36357092]\n",
      " [ 0.92949526  0.00218147]]\n",
      "At step 83250, TRAINING loss is 27.98254678115482\n",
      "At step 83500, TRAINING loss is 4727.275132606592\n",
      "At step 83750, TRAINING loss is 1407.0750977096232\n",
      "At step 84000, TRAINING loss is 34.53105950713223\n",
      "At step 84000, sampled x_0 is [[0.23291752 0.30597727]\n",
      " [1.7319397  0.22440555]]\n",
      "At step 84250, TRAINING loss is 1725.5273018072187\n",
      "At step 84500, TRAINING loss is 1502.1325405002049\n",
      "At step 84750, TRAINING loss is 83.19443511614159\n",
      "At step 85000, TRAINING loss is 1447.0784794335523\n",
      "At step 85000, sampled x_0 is [[-0.87764121 -0.78625647]\n",
      " [ 0.99259517 -0.80649973]]\n",
      "At step 85250, TRAINING loss is 4221.979198729066\n",
      "At step 85500, TRAINING loss is 2885.5757224594695\n",
      "At step 85750, TRAINING loss is 4598.526841649447\n",
      "At step 86000, TRAINING loss is 2272.2739510048377\n",
      "At step 86000, sampled x_0 is [[-1.01546342 -1.3721867 ]\n",
      " [-0.25064617  0.53433402]]\n",
      "At step 86250, TRAINING loss is 696.172947520972\n",
      "At step 86500, TRAINING loss is 1118.021715686654\n",
      "At step 86750, TRAINING loss is 3830.675204250835\n",
      "At step 87000, TRAINING loss is 73.49380011272271\n",
      "At step 87000, sampled x_0 is [[ 1.48253997 -0.43566838]\n",
      " [ 0.75134481  1.61595712]]\n",
      "At step 87250, TRAINING loss is 1005.6641649611847\n",
      "At step 87500, TRAINING loss is 1225.5759104764722\n",
      "At step 87750, TRAINING loss is 4199.057155936509\n",
      "At step 88000, TRAINING loss is 3407.734134914498\n",
      "At step 88000, sampled x_0 is [[ 1.64430981 -0.05203958]\n",
      " [ 0.86655011  0.25305553]]\n",
      "At step 88250, TRAINING loss is 526.7402060342414\n",
      "At step 88500, TRAINING loss is 3311.5749984448057\n",
      "At step 88750, TRAINING loss is 2514.474211831529\n",
      "At step 89000, TRAINING loss is 2010.405145437232\n",
      "At step 89000, sampled x_0 is [[-2.05836531 -0.4730706 ]\n",
      " [ 0.7940663   0.87174202]]\n",
      "At step 89250, TRAINING loss is 4140.967407656455\n",
      "At step 89500, TRAINING loss is 2093.451861545266\n",
      "At step 89750, TRAINING loss is 5040.575136166763\n",
      "At step 90000, TRAINING loss is 1506.2403636699746\n",
      "At step 90000, sampled x_0 is [[-0.92150891  2.05759048]\n",
      " [ 0.67709197  2.40772158]]\n",
      "At step 90250, TRAINING loss is 1264.932615069636\n",
      "At step 90500, TRAINING loss is 13.438659731300625\n",
      "At step 90750, TRAINING loss is 4739.330331723519\n",
      "At step 91000, TRAINING loss is 3616.1932576306967\n",
      "At step 91000, sampled x_0 is [[-0.32210775 -0.48493881]\n",
      " [ 1.04761431  0.61269412]]\n",
      "At step 91250, TRAINING loss is 18.078098231541805\n",
      "At step 91500, TRAINING loss is 2675.427397856093\n",
      "At step 91750, TRAINING loss is 311.9633983186858\n",
      "At step 92000, TRAINING loss is 113.62120131592836\n",
      "At step 92000, sampled x_0 is [[-1.02401849 -0.0053618 ]\n",
      " [ 0.58266339  1.25661757]]\n",
      "At step 92250, TRAINING loss is 1041.5037395968616\n",
      "At step 92500, TRAINING loss is 2068.5498424114076\n",
      "At step 92750, TRAINING loss is 944.2107682324124\n",
      "At step 93000, TRAINING loss is 2189.446624925448\n",
      "At step 93000, sampled x_0 is [[-0.41793565 -0.11493335]\n",
      " [-0.41619063  1.02255099]]\n",
      "At step 93250, TRAINING loss is 3830.8313696350533\n",
      "At step 93500, TRAINING loss is 3.6481983602125734\n",
      "At step 93750, TRAINING loss is 786.8789510765384\n",
      "At step 94000, TRAINING loss is 1608.4166613994853\n",
      "At step 94000, sampled x_0 is [[-0.58515641  0.31479423]\n",
      " [ 0.61491552  0.43771911]]\n",
      "At step 94250, TRAINING loss is 1028.080230515457\n",
      "At step 94500, TRAINING loss is 193.95318674878467\n",
      "At step 94750, TRAINING loss is 3930.0611070223968\n",
      "At step 95000, TRAINING loss is 1721.99609190132\n",
      "At step 95000, sampled x_0 is [[ 0.07070844 -0.30287036]\n",
      " [ 1.32923407  0.72802184]]\n",
      "At step 95250, TRAINING loss is 3792.7908342029446\n",
      "At step 95500, TRAINING loss is 464.03874372735487\n",
      "At step 95750, TRAINING loss is 2864.1998344499143\n",
      "At step 96000, TRAINING loss is 3333.1153198942643\n",
      "At step 96000, sampled x_0 is [[0.69663646 1.9541972 ]\n",
      " [0.5638999  1.08781659]]\n",
      "At step 96250, TRAINING loss is 687.3482782356431\n",
      "At step 96500, TRAINING loss is 376.1568743863712\n",
      "At step 96750, TRAINING loss is 31.689026216973534\n",
      "At step 97000, TRAINING loss is 1296.881445684625\n",
      "At step 97000, sampled x_0 is [[ 1.5337944  -1.23645191]\n",
      " [ 0.64623986  1.70694145]]\n",
      "At step 97250, TRAINING loss is 12.475316150335201\n",
      "At step 97500, TRAINING loss is 1206.2492316314574\n",
      "At step 97750, TRAINING loss is 4192.3807730286335\n",
      "At step 98000, TRAINING loss is 558.6411631684916\n",
      "At step 98000, sampled x_0 is [[-1.2584838  -0.01319127]\n",
      " [ 1.69989286  2.08942897]]\n",
      "At step 98250, TRAINING loss is 4148.284720893097\n",
      "At step 98500, TRAINING loss is 2001.9798143306957\n",
      "At step 98750, TRAINING loss is 1352.4076729375013\n",
      "At step 99000, TRAINING loss is 1511.6740686528105\n",
      "At step 99000, sampled x_0 is [[-2.0296461  -2.27155642]\n",
      " [ 0.69669272 -0.83453289]]\n",
      "At step 99250, TRAINING loss is 17.74872833479864\n",
      "At step 99500, TRAINING loss is 4609.486390431486\n",
      "At step 99750, TRAINING loss is 61.562039925700375\n",
      "At step 100000, TRAINING loss is 2.5276565277770104\n",
      "At step 100000, sampled x_0 is [[-1.78457987  1.74597783]\n",
      " [ 2.32716142  2.15777649]]\n",
      "At step 100250, TRAINING loss is 1715.7565311464386\n",
      "At step 100500, TRAINING loss is 1.7368040274248466\n",
      "At step 100750, TRAINING loss is 470.4532910019136\n",
      "At step 101000, TRAINING loss is 3587.667039955787\n",
      "At step 101000, sampled x_0 is [[ 0.55362024 -1.44140609]\n",
      " [ 1.27795856  1.81147442]]\n",
      "At step 101250, TRAINING loss is 432.5598605914612\n",
      "At step 101500, TRAINING loss is 2184.1660458398446\n",
      "At step 101750, TRAINING loss is 3368.8412596277267\n",
      "At step 102000, TRAINING loss is 401.74244787651037\n",
      "At step 102000, sampled x_0 is [[ 0.21628401 -0.51198402]\n",
      " [ 0.5086343   0.03811265]]\n",
      "At step 102250, TRAINING loss is 348.2783933764672\n",
      "At step 102500, TRAINING loss is 4628.226348345502\n",
      "At step 102750, TRAINING loss is 177.9150482398583\n",
      "At step 103000, TRAINING loss is 3612.884896329276\n",
      "At step 103000, sampled x_0 is [[ 0.06908995 -0.4024425 ]\n",
      " [ 0.47427322 -0.07708052]]\n",
      "At step 103250, TRAINING loss is 1116.5801916662936\n",
      "At step 103500, TRAINING loss is 3378.162168461319\n",
      "At step 103750, TRAINING loss is 2452.3852261176626\n",
      "At step 104000, TRAINING loss is 686.4389793759981\n",
      "At step 104000, sampled x_0 is [[ 0.46611324 -0.30455844]\n",
      " [ 1.47242441  0.73164551]]\n",
      "At step 104250, TRAINING loss is 924.952021149206\n",
      "At step 104500, TRAINING loss is 4437.2831496522795\n",
      "At step 104750, TRAINING loss is 2069.88529141802\n",
      "At step 105000, TRAINING loss is 494.6160405534947\n",
      "At step 105000, sampled x_0 is [[1.60346103 0.61264837]\n",
      " [0.96995242 1.32738963]]\n",
      "At step 105250, TRAINING loss is 41.41486054813308\n",
      "At step 105500, TRAINING loss is 223.17546338518105\n",
      "At step 105750, TRAINING loss is 1511.3896674859438\n",
      "At step 106000, TRAINING loss is 2059.5330535327166\n",
      "At step 106000, sampled x_0 is [[-0.68813367 -0.4004515 ]\n",
      " [-1.17157766  1.65340772]]\n",
      "At step 106250, TRAINING loss is 4632.482813792469\n",
      "At step 106500, TRAINING loss is 2048.192815883613\n",
      "At step 106750, TRAINING loss is 2634.2925086375312\n",
      "At step 107000, TRAINING loss is 64.67899536712387\n",
      "At step 107000, sampled x_0 is [[0.57695502 0.24689565]\n",
      " [1.30442012 1.37631065]]\n",
      "At step 107250, TRAINING loss is 1911.522034710986\n",
      "At step 107500, TRAINING loss is 39.91016771756929\n",
      "At step 107750, TRAINING loss is 1627.2120401508676\n",
      "At step 108000, TRAINING loss is 2497.7149034601807\n",
      "At step 108000, sampled x_0 is [[-0.31044278  0.89008117]\n",
      " [ 0.2083049   0.86424895]]\n",
      "At step 108250, TRAINING loss is 868.0152666841662\n",
      "At step 108500, TRAINING loss is 2337.0867485780236\n",
      "At step 108750, TRAINING loss is 4607.855027786095\n",
      "At step 109000, TRAINING loss is 1873.632166321132\n",
      "At step 109000, sampled x_0 is [[ 0.16238717 -0.323319  ]\n",
      " [ 0.23575398 -1.5021016 ]]\n",
      "At step 109250, TRAINING loss is 4611.67183356253\n",
      "At step 109500, TRAINING loss is 50.35434166661717\n",
      "At step 109750, TRAINING loss is 2203.6436499244087\n",
      "At step 110000, TRAINING loss is 617.2340241606371\n",
      "At step 110000, sampled x_0 is [[ 0.68937133  0.13484928]\n",
      " [-0.58573783  2.34538633]]\n",
      "At step 110250, TRAINING loss is 556.4460195419679\n",
      "At step 110500, TRAINING loss is 1985.04095308866\n",
      "At step 110750, TRAINING loss is 225.67376955432613\n",
      "At step 111000, TRAINING loss is 2624.2967705781025\n",
      "At step 111000, sampled x_0 is [[ 1.88554503 -0.19702581]\n",
      " [ 2.08410043  1.59856182]]\n",
      "At step 111250, TRAINING loss is 13.040279695452131\n",
      "At step 111500, TRAINING loss is 204.42130204991832\n",
      "At step 111750, TRAINING loss is 65.65033817132169\n",
      "At step 112000, TRAINING loss is 1956.7672578962045\n",
      "At step 112000, sampled x_0 is [[-0.56152724 -0.3192723 ]\n",
      " [ 1.36025125  0.86817408]]\n",
      "At step 112250, TRAINING loss is 66.35732458179987\n",
      "At step 112500, TRAINING loss is 78.15907290132856\n",
      "At step 112750, TRAINING loss is 374.3698186565007\n",
      "At step 113000, TRAINING loss is 444.9130803954099\n",
      "At step 113000, sampled x_0 is [[-0.66972951  0.45968729]\n",
      " [ 1.54370673  1.61787091]]\n",
      "At step 113250, TRAINING loss is 134.34667193034954\n",
      "At step 113500, TRAINING loss is 4601.416731570078\n",
      "At step 113750, TRAINING loss is 3946.6976191875992\n",
      "At step 114000, TRAINING loss is 121.4094060194752\n",
      "At step 114000, sampled x_0 is [[1.81562512 0.13488015]\n",
      " [2.18077491 0.12978461]]\n",
      "At step 114250, TRAINING loss is 365.78640876551106\n",
      "At step 114500, TRAINING loss is 1072.8653807132773\n",
      "At step 114750, TRAINING loss is 3042.5626119715266\n",
      "At step 115000, TRAINING loss is 1682.819162351979\n",
      "At step 115000, sampled x_0 is [[0.36962933 0.73296181]\n",
      " [0.35839362 1.63310581]]\n",
      "At step 115250, TRAINING loss is 4842.0667190861595\n",
      "At step 115500, TRAINING loss is 4907.386061275018\n",
      "At step 115750, TRAINING loss is 1078.1227782863311\n",
      "At step 116000, TRAINING loss is 122.47967681850102\n",
      "At step 116000, sampled x_0 is [[ 1.11814541  0.86099137]\n",
      " [-0.13133892  2.7343849 ]]\n",
      "At step 116250, TRAINING loss is 1313.9770215252697\n",
      "At step 116500, TRAINING loss is 2951.009939815227\n",
      "At step 116750, TRAINING loss is 620.1741816181378\n",
      "At step 117000, TRAINING loss is 1900.7316193336105\n",
      "At step 117000, sampled x_0 is [[ 2.0403403  -0.61285556]\n",
      " [-1.58310343  1.63495579]]\n",
      "At step 117250, TRAINING loss is 1036.7641166992664\n",
      "At step 117500, TRAINING loss is 457.3919436542535\n",
      "At step 117750, TRAINING loss is 117.57188772798432\n",
      "At step 118000, TRAINING loss is 4716.224175900674\n",
      "At step 118000, sampled x_0 is [[-0.9412494  -0.06639811]\n",
      " [ 0.80481708 -0.13045413]]\n",
      "At step 118250, TRAINING loss is 54.771923443440215\n",
      "At step 118500, TRAINING loss is 2510.2188494443353\n",
      "At step 118750, TRAINING loss is 77.28132500672487\n",
      "At step 119000, TRAINING loss is 2583.4956183617232\n",
      "At step 119000, sampled x_0 is [[ 1.0559208  -0.42587476]\n",
      " [ 0.61777037  0.22209003]]\n",
      "At step 119250, TRAINING loss is 2082.998532732735\n",
      "At step 119500, TRAINING loss is 281.9592514978677\n",
      "At step 119750, TRAINING loss is 963.7533226302999\n",
      "At step 120000, TRAINING loss is 4250.268971165477\n",
      "At step 120000, sampled x_0 is [[ 0.94448341 -0.65360208]\n",
      " [ 3.08215511  0.19804271]]\n",
      "At step 120250, TRAINING loss is 3828.188305006227\n",
      "At step 120500, TRAINING loss is 4414.571514859143\n",
      "At step 120750, TRAINING loss is 2751.8829649170457\n",
      "At step 121000, TRAINING loss is 3586.1509748686726\n",
      "At step 121000, sampled x_0 is [[0.52048191 0.38241789]\n",
      " [1.17991532 0.2188868 ]]\n",
      "At step 121250, TRAINING loss is 3902.7173747767074\n",
      "At step 121500, TRAINING loss is 4012.3966205758816\n",
      "At step 121750, TRAINING loss is 4744.587650155404\n",
      "At step 122000, TRAINING loss is 2081.049545336202\n",
      "At step 122000, sampled x_0 is [[ 0.32254027 -0.45959904]\n",
      " [-0.3636378   0.35924118]]\n",
      "At step 122250, TRAINING loss is 1071.7137844435197\n",
      "At step 122500, TRAINING loss is 844.3165188882933\n",
      "At step 122750, TRAINING loss is 2805.5010536687646\n",
      "At step 123000, TRAINING loss is 4053.9302311001243\n",
      "At step 123000, sampled x_0 is [[-1.66765679  1.16614039]\n",
      " [-0.40817981  0.59867933]]\n",
      "At step 123250, TRAINING loss is 72.74726140571155\n",
      "At step 123500, TRAINING loss is 71.64608641871911\n",
      "At step 123750, TRAINING loss is 1396.008882186824\n",
      "At step 124000, TRAINING loss is 510.76112998333656\n",
      "At step 124000, sampled x_0 is [[-2.18540287 -0.56064862]\n",
      " [ 0.3642874   0.72929157]]\n",
      "At step 124250, TRAINING loss is 4262.460882179644\n",
      "At step 124500, TRAINING loss is 2250.8698065629515\n",
      "At step 124750, TRAINING loss is 2613.38419440918\n",
      "At step 125000, TRAINING loss is 1457.3172156077542\n",
      "At step 125000, sampled x_0 is [[ 0.50961176  0.15702942]\n",
      " [ 1.16589874 -0.16519531]]\n",
      "At step 125250, TRAINING loss is 4656.316828632619\n",
      "At step 125500, TRAINING loss is 2259.1045387517806\n",
      "At step 125750, TRAINING loss is 3156.211480606036\n",
      "At step 126000, TRAINING loss is 367.9232036040049\n",
      "At step 126000, sampled x_0 is [[0.98364591 0.5710629 ]\n",
      " [1.94692454 0.83319161]]\n",
      "At step 126250, TRAINING loss is 37.19848388323135\n",
      "At step 126500, TRAINING loss is 2397.757756977845\n",
      "At step 126750, TRAINING loss is 4924.955936913748\n",
      "At step 127000, TRAINING loss is 2923.7787892935357\n",
      "At step 127000, sampled x_0 is [[-0.74976244  0.68786026]\n",
      " [ 2.78726172  1.64549873]]\n",
      "At step 127250, TRAINING loss is 1452.6823280748104\n",
      "At step 127500, TRAINING loss is 262.6566342293756\n",
      "At step 127750, TRAINING loss is 223.0408083682005\n",
      "At step 128000, TRAINING loss is 252.37498183951286\n",
      "At step 128000, sampled x_0 is [[ 0.11955472 -0.68764943]\n",
      " [ 1.97769032 -0.43243436]]\n",
      "At step 128250, TRAINING loss is 95.69451821510052\n",
      "At step 128500, TRAINING loss is 295.5359913045423\n",
      "At step 128750, TRAINING loss is 3488.3256862443204\n",
      "At step 129000, TRAINING loss is 4126.846274472777\n",
      "At step 129000, sampled x_0 is [[1.64558499 0.84015898]\n",
      " [0.44582839 1.64001029]]\n",
      "At step 129250, TRAINING loss is 1403.5750359012754\n",
      "At step 129500, TRAINING loss is 1784.9901387622476\n",
      "At step 129750, TRAINING loss is 2470.5935099967273\n",
      "At step 130000, TRAINING loss is 186.87418923497654\n",
      "At step 130000, sampled x_0 is [[-1.24984677  1.74157795]\n",
      " [ 1.75945245  0.43147037]]\n",
      "At step 130250, TRAINING loss is 119.79942087272491\n",
      "At step 130500, TRAINING loss is 907.6896489164386\n",
      "At step 130750, TRAINING loss is 321.2708188818432\n",
      "At step 131000, TRAINING loss is 1928.0678071615034\n",
      "At step 131000, sampled x_0 is [[-2.21344455  2.00130905]\n",
      " [-0.49862844  2.32851722]]\n",
      "At step 131250, TRAINING loss is 334.1886430374361\n",
      "At step 131500, TRAINING loss is 2223.7849190825\n",
      "At step 131750, TRAINING loss is 4054.1334061907146\n",
      "At step 132000, TRAINING loss is 187.1654477304329\n",
      "At step 132000, sampled x_0 is [[-0.48640884 -0.66301324]\n",
      " [ 1.24757609  2.18317067]]\n",
      "At step 132250, TRAINING loss is 2194.095475214815\n",
      "At step 132500, TRAINING loss is 209.6207349387539\n",
      "At step 132750, TRAINING loss is 109.8919236417673\n",
      "At step 133000, TRAINING loss is 0.4985510183442132\n",
      "At step 133000, sampled x_0 is [[-0.23005762  0.29766102]\n",
      " [ 2.61976289  1.46250665]]\n",
      "At step 133250, TRAINING loss is 1520.3130890792916\n",
      "At step 133500, TRAINING loss is 492.01644058255914\n",
      "At step 133750, TRAINING loss is 12.878129135300902\n",
      "At step 134000, TRAINING loss is 463.53096273744745\n",
      "At step 134000, sampled x_0 is [[-0.05957424  0.25482956]\n",
      " [ 1.68778053  1.39484119]]\n",
      "At step 134250, TRAINING loss is 850.6645891772371\n",
      "At step 134500, TRAINING loss is 3111.3485541503605\n",
      "At step 134750, TRAINING loss is 2356.479729463844\n",
      "At step 135000, TRAINING loss is 596.0026901964436\n",
      "At step 135000, sampled x_0 is [[ 1.24577686 -0.53854653]\n",
      " [ 0.224171    0.5563942 ]]\n",
      "At step 135250, TRAINING loss is 2168.3290122136123\n",
      "At step 135500, TRAINING loss is 4408.056864142965\n",
      "At step 135750, TRAINING loss is 727.0947732750315\n",
      "At step 136000, TRAINING loss is 3549.760819660374\n",
      "At step 136000, sampled x_0 is [[ 0.07713583 -0.09088317]\n",
      " [-0.62563311  1.15884019]]\n",
      "At step 136250, TRAINING loss is 1200.5287132447893\n",
      "At step 136500, TRAINING loss is 2580.733484697679\n",
      "At step 136750, TRAINING loss is 1720.2033656284502\n",
      "At step 137000, TRAINING loss is 130.1061597224642\n",
      "At step 137000, sampled x_0 is [[-1.85135362 -0.85095869]\n",
      " [ 1.52192661  1.26568178]]\n",
      "At step 137250, TRAINING loss is 206.9239039520771\n",
      "At step 137500, TRAINING loss is 2994.720255789577\n",
      "At step 137750, TRAINING loss is 3.6322145805217385\n",
      "At step 138000, TRAINING loss is 931.2446632330048\n",
      "At step 138000, sampled x_0 is [[ 1.3942645  -0.57941307]\n",
      " [ 0.47902418  0.98250274]]\n",
      "At step 138250, TRAINING loss is 884.8912249618966\n",
      "At step 138500, TRAINING loss is 240.47772666124712\n",
      "At step 138750, TRAINING loss is 2753.7467639765955\n",
      "At step 139000, TRAINING loss is 3104.540229576275\n",
      "At step 139000, sampled x_0 is [[-0.09378936 -1.47913756]\n",
      " [-1.01250327  0.62498091]]\n",
      "At step 139250, TRAINING loss is 3702.0216452012673\n",
      "At step 139500, TRAINING loss is 74.82092322525202\n",
      "At step 139750, TRAINING loss is 452.17142969702854\n",
      "At step 140000, TRAINING loss is 1162.1898278230362\n",
      "At step 140000, sampled x_0 is [[-0.02466606 -0.3400603 ]\n",
      " [ 1.03419067  1.24314537]]\n",
      "At step 140250, TRAINING loss is 4150.102846523836\n",
      "At step 140500, TRAINING loss is 414.80775117867756\n",
      "At step 140750, TRAINING loss is 1208.6965839143209\n",
      "At step 141000, TRAINING loss is 200.7447207985348\n",
      "At step 141000, sampled x_0 is [[0.97619717 0.6838544 ]\n",
      " [0.73667252 0.55284028]]\n",
      "At step 141250, TRAINING loss is 250.40391659844525\n",
      "At step 141500, TRAINING loss is 931.0850838307615\n",
      "At step 141750, TRAINING loss is 2334.8276347073424\n",
      "At step 142000, TRAINING loss is 68.17044312202327\n",
      "At step 142000, sampled x_0 is [[0.51184013 0.04332067]\n",
      " [0.91615541 0.62433824]]\n",
      "At step 142250, TRAINING loss is 4487.916924149678\n",
      "At step 142500, TRAINING loss is 1384.8998141601621\n",
      "At step 142750, TRAINING loss is 1060.517719236501\n",
      "At step 143000, TRAINING loss is 1678.7818245236929\n",
      "At step 143000, sampled x_0 is [[ 0.77114747 -0.43943676]\n",
      " [ 0.32741768  1.89245984]]\n",
      "At step 143250, TRAINING loss is 11.735357400553273\n",
      "At step 143500, TRAINING loss is 4929.357604176125\n",
      "At step 143750, TRAINING loss is 407.09865872792045\n",
      "At step 144000, TRAINING loss is 9.956099181076421\n",
      "At step 144000, sampled x_0 is [[ 1.1699645  -1.23918819]\n",
      " [ 0.51123427  2.20174717]]\n",
      "At step 144250, TRAINING loss is 2863.097678413016\n",
      "At step 144500, TRAINING loss is 913.1190814674928\n",
      "At step 144750, TRAINING loss is 2486.7911783488116\n",
      "At step 145000, TRAINING loss is 292.75735450270747\n",
      "At step 145000, sampled x_0 is [[-1.09605089 -0.23808157]\n",
      " [ 0.61718996 -0.25538903]]\n",
      "At step 145250, TRAINING loss is 189.28812214039493\n",
      "At step 145500, TRAINING loss is 2779.78985310571\n",
      "At step 145750, TRAINING loss is 2071.501298359946\n",
      "At step 146000, TRAINING loss is 2441.1461414210007\n",
      "At step 146000, sampled x_0 is [[-9.17433259e-04 -2.58919743e-01]\n",
      " [ 2.68980145e+00  1.27764559e+00]]\n",
      "At step 146250, TRAINING loss is 3141.901515203039\n",
      "At step 146500, TRAINING loss is 21.04676790870569\n",
      "At step 146750, TRAINING loss is 12.88244389113048\n",
      "At step 147000, TRAINING loss is 480.10348279132506\n",
      "At step 147000, sampled x_0 is [[ 0.43081177 -0.34102316]\n",
      " [ 1.79280632 -0.22324575]]\n",
      "At step 147250, TRAINING loss is 148.18671671825405\n",
      "At step 147500, TRAINING loss is 620.0497198914219\n",
      "At step 147750, TRAINING loss is 116.65711674800866\n",
      "At step 148000, TRAINING loss is 4753.6063423942815\n",
      "At step 148000, sampled x_0 is [[-0.50626554 -0.02513191]\n",
      " [ 1.17684749  1.42057744]]\n",
      "At step 148250, TRAINING loss is 3652.791195565198\n",
      "At step 148500, TRAINING loss is 66.2646250576474\n",
      "At step 148750, TRAINING loss is 244.18638399622233\n",
      "At step 149000, TRAINING loss is 2071.746364167194\n",
      "At step 149000, sampled x_0 is [[0.27928267 1.20758248]\n",
      " [0.66324396 0.32195122]]\n",
      "At step 149250, TRAINING loss is 252.93524173182146\n",
      "At step 149500, TRAINING loss is 429.5566720951293\n",
      "At step 149750, TRAINING loss is 52.99334562282279\n",
      "At step 150000, TRAINING loss is 22.01936027612799\n",
      "At step 150000, sampled x_0 is [[0.67049338 1.06430602]\n",
      " [1.5789038  1.57707797]]\n",
      "At step 150250, TRAINING loss is 3229.5613024203785\n",
      "At step 150500, TRAINING loss is 479.755588240938\n",
      "At step 150750, TRAINING loss is 892.9021577713547\n",
      "At step 151000, TRAINING loss is 3681.379111314968\n",
      "At step 151000, sampled x_0 is [[ 1.72259495 -1.52589754]\n",
      " [ 2.52733616  0.06031644]]\n",
      "At step 151250, TRAINING loss is 3829.722667024047\n",
      "At step 151500, TRAINING loss is 107.70593311982731\n",
      "At step 151750, TRAINING loss is 1978.3949748032758\n",
      "At step 152000, TRAINING loss is 97.20140351370372\n",
      "At step 152000, sampled x_0 is [[-0.72241191 -0.08220904]\n",
      " [ 0.3768776   1.44440859]]\n",
      "At step 152250, TRAINING loss is 3049.1786467556285\n",
      "At step 152500, TRAINING loss is 2254.286240316094\n",
      "At step 152750, TRAINING loss is 35.71475650697808\n",
      "At step 153000, TRAINING loss is 148.14868368341507\n",
      "At step 153000, sampled x_0 is [[ 0.82790168 -0.18022638]\n",
      " [-0.20010489  1.93304118]]\n",
      "At step 153250, TRAINING loss is 4398.642902598933\n",
      "At step 153500, TRAINING loss is 325.9211233045496\n",
      "At step 153750, TRAINING loss is 90.67204171330289\n",
      "At step 154000, TRAINING loss is 2534.3866884153963\n",
      "At step 154000, sampled x_0 is [[-1.2292457   0.91694165]\n",
      " [ 1.01266673 -0.27243538]]\n",
      "At step 154250, TRAINING loss is 3108.0463497841806\n",
      "At step 154500, TRAINING loss is 2686.6546957125993\n",
      "At step 154750, TRAINING loss is 438.22478825401106\n",
      "At step 155000, TRAINING loss is 2195.453760505775\n",
      "At step 155000, sampled x_0 is [[-1.64339339  1.68695537]\n",
      " [ 1.28943591  1.93076465]]\n",
      "At step 155250, TRAINING loss is 2669.935882410953\n",
      "At step 155500, TRAINING loss is 915.1525867419116\n",
      "At step 155750, TRAINING loss is 2981.4619232358964\n",
      "At step 156000, TRAINING loss is 250.3063802202605\n",
      "At step 156000, sampled x_0 is [[-0.43891457 -0.52657782]\n",
      " [ 1.26722057  0.03021436]]\n",
      "At step 156250, TRAINING loss is 725.0297136257597\n",
      "At step 156500, TRAINING loss is 233.91011683459092\n",
      "At step 156750, TRAINING loss is 4822.4426639381545\n",
      "At step 157000, TRAINING loss is 1874.2129406346075\n",
      "At step 157000, sampled x_0 is [[ 0.88200329 -0.35175559]\n",
      " [ 2.33363208  1.56458776]]\n",
      "At step 157250, TRAINING loss is 2492.6548836185975\n",
      "At step 157500, TRAINING loss is 4208.046364693262\n",
      "At step 157750, TRAINING loss is 1789.1177293464857\n",
      "At step 158000, TRAINING loss is 55.90124886060194\n",
      "At step 158000, sampled x_0 is [[-2.49309893 -0.02566123]\n",
      " [ 2.387307    2.54586669]]\n",
      "At step 158250, TRAINING loss is 606.7702308922326\n",
      "At step 158500, TRAINING loss is 113.51698525552854\n",
      "At step 158750, TRAINING loss is 3306.1422144061244\n",
      "At step 159000, TRAINING loss is 5.149121483718616\n",
      "At step 159000, sampled x_0 is [[-1.03474808 -0.7491229 ]\n",
      " [-1.11188126  0.17976591]]\n",
      "At step 159250, TRAINING loss is 1411.740947994123\n",
      "At step 159500, TRAINING loss is 185.92128863042512\n",
      "At step 159750, TRAINING loss is 2158.6201865060775\n",
      "At step 160000, TRAINING loss is 2468.646450334807\n",
      "At step 160000, sampled x_0 is [[ 1.71176287  1.09198074]\n",
      " [ 0.98711815 -0.68306581]]\n",
      "At step 160250, TRAINING loss is 945.2330492657854\n",
      "At step 160500, TRAINING loss is 3979.867013789918\n",
      "At step 160750, TRAINING loss is 1739.2820446188925\n",
      "At step 161000, TRAINING loss is 4136.393445663534\n",
      "At step 161000, sampled x_0 is [[0.48154694 0.37625672]\n",
      " [1.29544054 2.09206936]]\n",
      "At step 161250, TRAINING loss is 139.3218122915079\n",
      "At step 161500, TRAINING loss is 3150.791961118152\n",
      "At step 161750, TRAINING loss is 70.43999892065615\n",
      "At step 162000, TRAINING loss is 6.414060232160546\n",
      "At step 162000, sampled x_0 is [[ 0.1482176  -0.32388184]\n",
      " [ 0.81300707  1.14234163]]\n",
      "At step 162250, TRAINING loss is 5.569906144520018\n",
      "At step 162500, TRAINING loss is 4854.505946348334\n",
      "At step 162750, TRAINING loss is 3479.3251070204797\n",
      "At step 163000, TRAINING loss is 346.1614571485153\n",
      "At step 163000, sampled x_0 is [[ 0.53629736 -0.87822688]\n",
      " [-0.31643668  2.12128613]]\n",
      "At step 163250, TRAINING loss is 2746.743069090675\n",
      "At step 163500, TRAINING loss is 43.254487199736374\n",
      "At step 163750, TRAINING loss is 402.4689430515815\n",
      "At step 164000, TRAINING loss is 1736.8050706946551\n",
      "At step 164000, sampled x_0 is [[-2.93309464 -0.60426936]\n",
      " [ 2.42519937  0.20341035]]\n",
      "At step 164250, TRAINING loss is 3137.530503615977\n",
      "At step 164500, TRAINING loss is 380.93619373928175\n",
      "At step 164750, TRAINING loss is 1048.6730381358566\n",
      "At step 165000, TRAINING loss is 450.91451892022843\n",
      "At step 165000, sampled x_0 is [[-0.6903475  -0.71159656]\n",
      " [ 0.61984727  0.96948899]]\n",
      "At step 165250, TRAINING loss is 941.5783532619428\n",
      "At step 165500, TRAINING loss is 3266.6531585207576\n",
      "At step 165750, TRAINING loss is 4591.610629196883\n",
      "At step 166000, TRAINING loss is 2731.7641863745457\n",
      "At step 166000, sampled x_0 is [[-0.82362875 -1.85890992]\n",
      " [ 1.1122316   1.00459361]]\n",
      "At step 166250, TRAINING loss is 4344.395189531711\n",
      "At step 166500, TRAINING loss is 1862.5437657049633\n",
      "At step 166750, TRAINING loss is 5.17367722608347\n",
      "At step 167000, TRAINING loss is 511.2011500095379\n",
      "At step 167000, sampled x_0 is [[ 0.57494624 -1.073666  ]\n",
      " [ 0.58653513  0.82387439]]\n",
      "At step 167250, TRAINING loss is 57.430484874201085\n",
      "At step 167500, TRAINING loss is 879.1860338739377\n",
      "At step 167750, TRAINING loss is 192.78185604284673\n",
      "At step 168000, TRAINING loss is 80.33917956084565\n",
      "At step 168000, sampled x_0 is [[1.62635621 0.49288522]\n",
      " [0.38457912 0.56315476]]\n",
      "At step 168250, TRAINING loss is 1667.4905510040753\n",
      "At step 168500, TRAINING loss is 1972.9331647050783\n",
      "At step 168750, TRAINING loss is 72.23967220309507\n",
      "At step 169000, TRAINING loss is 9.319846556498312\n",
      "At step 169000, sampled x_0 is [[-0.34472894 -0.08118927]\n",
      " [ 0.72439017  0.41927219]]\n",
      "At step 169250, TRAINING loss is 2920.6181930219454\n",
      "At step 169500, TRAINING loss is 3746.026059942739\n",
      "At step 169750, TRAINING loss is 13.47484676450054\n",
      "At step 170000, TRAINING loss is 342.50774368336704\n",
      "At step 170000, sampled x_0 is [[-0.28074016  2.0493246 ]\n",
      " [ 1.34452964  1.11093429]]\n",
      "At step 170250, TRAINING loss is 79.27576418427842\n",
      "At step 170500, TRAINING loss is 2916.6433504197885\n",
      "At step 170750, TRAINING loss is 162.3565070581362\n",
      "At step 171000, TRAINING loss is 1905.5440489203309\n",
      "At step 171000, sampled x_0 is [[-0.82104878 -0.47317281]\n",
      " [ 1.51387981  0.34521935]]\n",
      "At step 171250, TRAINING loss is 1603.9056617935057\n",
      "At step 171500, TRAINING loss is 4852.353615245092\n",
      "At step 171750, TRAINING loss is 377.24411960511657\n",
      "At step 172000, TRAINING loss is 1593.9589970140237\n",
      "At step 172000, sampled x_0 is [[ 0.86182538 -0.54439362]\n",
      " [ 0.56345046  1.46284587]]\n",
      "At step 172250, TRAINING loss is 242.55374171036135\n",
      "At step 172500, TRAINING loss is 1232.2143409277082\n",
      "At step 172750, TRAINING loss is 423.8950006182673\n",
      "At step 173000, TRAINING loss is 3101.1288305905246\n",
      "At step 173000, sampled x_0 is [[ 0.31270236 -1.23480709]\n",
      " [ 0.46320798  0.57069249]]\n",
      "At step 173250, TRAINING loss is 1833.2688824792253\n",
      "At step 173500, TRAINING loss is 2570.6521279464832\n",
      "At step 173750, TRAINING loss is 1279.1029807040234\n",
      "At step 174000, TRAINING loss is 90.73338592400133\n",
      "At step 174000, sampled x_0 is [[-1.14080289 -0.15661863]\n",
      " [ 0.03990784  0.43133692]]\n",
      "At step 174250, TRAINING loss is 4255.141267256671\n",
      "At step 174500, TRAINING loss is 3570.421696136945\n",
      "At step 174750, TRAINING loss is 253.82059584868236\n",
      "At step 175000, TRAINING loss is 4860.555967889178\n",
      "At step 175000, sampled x_0 is [[ 2.39184686 -0.31453551]\n",
      " [-0.35900109  1.37843484]]\n",
      "At step 175250, TRAINING loss is 281.9453432129594\n",
      "At step 175500, TRAINING loss is 3647.2048782212396\n",
      "At step 175750, TRAINING loss is 104.17074943086462\n",
      "At step 176000, TRAINING loss is 111.61524262470975\n",
      "At step 176000, sampled x_0 is [[-0.33755818  1.06157451]\n",
      " [ 1.37254964  1.34652703]]\n",
      "At step 176250, TRAINING loss is 2406.0383202905846\n",
      "At step 176500, TRAINING loss is 2420.1494661970764\n",
      "At step 176750, TRAINING loss is 3127.2173507222337\n",
      "At step 177000, TRAINING loss is 1293.4816787573436\n",
      "At step 177000, sampled x_0 is [[ 0.36870125 -1.1940993 ]\n",
      " [ 0.48161626  0.51170116]]\n",
      "At step 177250, TRAINING loss is 2375.197579227715\n",
      "At step 177500, TRAINING loss is 3135.8627377611\n",
      "At step 177750, TRAINING loss is 1245.8781164000764\n",
      "At step 178000, TRAINING loss is 2.0652031632107497\n",
      "At step 178000, sampled x_0 is [[0.19171833 1.42328858]\n",
      " [1.91184789 1.97449075]]\n",
      "At step 178250, TRAINING loss is 320.6488611070064\n",
      "At step 178500, TRAINING loss is 3179.475402275626\n",
      "At step 178750, TRAINING loss is 41.39557996507905\n",
      "At step 179000, TRAINING loss is 3197.2204703870007\n",
      "At step 179000, sampled x_0 is [[-0.10838277 -1.28937901]\n",
      " [ 0.81785789  1.43531751]]\n",
      "At step 179250, TRAINING loss is 1027.1570564414833\n",
      "At step 179500, TRAINING loss is 1840.9172067214352\n",
      "At step 179750, TRAINING loss is 785.742834287564\n",
      "At step 180000, TRAINING loss is 17.42263682547086\n",
      "At step 180000, sampled x_0 is [[-0.91094285  0.33444282]\n",
      " [ 0.2531751   0.49926115]]\n",
      "At step 180250, TRAINING loss is 3545.99581254246\n",
      "At step 180500, TRAINING loss is 89.37086446555612\n",
      "At step 180750, TRAINING loss is 2262.838702354801\n",
      "At step 181000, TRAINING loss is 904.2514431906205\n",
      "At step 181000, sampled x_0 is [[ 1.53426555 -0.84233995]\n",
      " [ 0.13447649  1.26842546]]\n",
      "At step 181250, TRAINING loss is 213.58007336665335\n",
      "At step 181500, TRAINING loss is 1550.7827642711136\n",
      "At step 181750, TRAINING loss is 4038.523981373136\n",
      "At step 182000, TRAINING loss is 3005.1528854373364\n",
      "At step 182000, sampled x_0 is [[-0.05197189  1.12809307]\n",
      " [ 0.39237935  0.58739524]]\n",
      "At step 182250, TRAINING loss is 4362.938649656398\n",
      "At step 182500, TRAINING loss is 51.72503277604768\n",
      "At step 182750, TRAINING loss is 310.75631674446936\n",
      "At step 183000, TRAINING loss is 492.0517189272545\n",
      "At step 183000, sampled x_0 is [[-0.4434386  -0.81809664]\n",
      " [-0.90103748 -0.16918685]]\n",
      "At step 183250, TRAINING loss is 4016.1773405796575\n",
      "At step 183500, TRAINING loss is 451.022368592511\n",
      "At step 183750, TRAINING loss is 1518.3244733163256\n",
      "At step 184000, TRAINING loss is 2677.9870491456436\n",
      "At step 184000, sampled x_0 is [[0.30056166 0.62265529]\n",
      " [0.14219557 1.68411509]]\n",
      "At step 184250, TRAINING loss is 3511.8305690617735\n",
      "At step 184500, TRAINING loss is 77.72439386428897\n",
      "At step 184750, TRAINING loss is 378.78154251305637\n",
      "At step 185000, TRAINING loss is 2563.544557854577\n",
      "At step 185000, sampled x_0 is [[-0.78059106  0.04978993]\n",
      " [ 1.57474743  0.8788645 ]]\n",
      "At step 185250, TRAINING loss is 1647.2381600017516\n",
      "At step 185500, TRAINING loss is 3643.58328367132\n",
      "At step 185750, TRAINING loss is 2535.137762895697\n",
      "At step 186000, TRAINING loss is 932.5194987412833\n",
      "At step 186000, sampled x_0 is [[ 0.15125436 -0.66743264]\n",
      " [ 1.3135213   1.74877912]]\n",
      "At step 186250, TRAINING loss is 100.83521405787255\n",
      "At step 186500, TRAINING loss is 38.586459483215265\n",
      "At step 186750, TRAINING loss is 0.4888621308288841\n",
      "At step 187000, TRAINING loss is 140.95240270624623\n",
      "At step 187000, sampled x_0 is [[ 1.19622354  0.46222406]\n",
      " [-1.0448113   0.68754096]]\n",
      "At step 187250, TRAINING loss is 117.23236567807554\n",
      "At step 187500, TRAINING loss is 392.4309865920219\n",
      "At step 187750, TRAINING loss is 290.648285390757\n",
      "At step 188000, TRAINING loss is 3920.3072318480495\n",
      "At step 188000, sampled x_0 is [[-0.70596043 -2.03060561]\n",
      " [-0.06381282  1.21496613]]\n",
      "At step 188250, TRAINING loss is 943.0806514521428\n",
      "At step 188500, TRAINING loss is 4692.857670814178\n",
      "At step 188750, TRAINING loss is 2083.541267761145\n",
      "At step 189000, TRAINING loss is 4754.211386551095\n",
      "At step 189000, sampled x_0 is [[-1.30461919  1.00071769]\n",
      " [ 0.68672471  2.73354626]]\n",
      "At step 189250, TRAINING loss is 1103.5070496013943\n",
      "At step 189500, TRAINING loss is 4753.787935735472\n",
      "At step 189750, TRAINING loss is 36.216150009986904\n",
      "At step 190000, TRAINING loss is 1764.4275844409267\n",
      "At step 190000, sampled x_0 is [[-0.93999518  0.81564818]\n",
      " [ 2.5988223  -0.91779058]]\n",
      "At step 190250, TRAINING loss is 97.69235578236007\n",
      "At step 190500, TRAINING loss is 168.01428274684963\n",
      "At step 190750, TRAINING loss is 2409.881409687962\n",
      "At step 191000, TRAINING loss is 498.27084969263393\n",
      "At step 191000, sampled x_0 is [[-0.64036387  0.78323285]\n",
      " [ 1.38162616  0.87937378]]\n",
      "At step 191250, TRAINING loss is 4179.883864564812\n",
      "At step 191500, TRAINING loss is 339.20029319237995\n",
      "At step 191750, TRAINING loss is 733.6905501784952\n",
      "At step 192000, TRAINING loss is 2.7078718199954355\n",
      "At step 192000, sampled x_0 is [[0.04306337 0.5703067 ]\n",
      " [2.14433027 1.05707246]]\n",
      "At step 192250, TRAINING loss is 4945.345819060438\n",
      "At step 192500, TRAINING loss is 117.22249703178291\n",
      "At step 192750, TRAINING loss is 1531.7431253551767\n",
      "At step 193000, TRAINING loss is 2.1971379065331265\n",
      "At step 193000, sampled x_0 is [[-1.95945049  0.68932516]\n",
      " [ 0.81439721  0.08893793]]\n",
      "At step 193250, TRAINING loss is 8.119365374144078\n",
      "At step 193500, TRAINING loss is 915.7935747459292\n",
      "At step 193750, TRAINING loss is 3919.7739444110452\n",
      "At step 194000, TRAINING loss is 2757.7347768485934\n",
      "At step 194000, sampled x_0 is [[-2.47135873 -2.40883823]\n",
      " [-0.10748305 -0.17838879]]\n",
      "At step 194250, TRAINING loss is 24.208618119428124\n",
      "At step 194500, TRAINING loss is 2035.0295329865755\n",
      "At step 194750, TRAINING loss is 4034.037870107311\n",
      "At step 195000, TRAINING loss is 18.017253980628666\n",
      "At step 195000, sampled x_0 is [[-1.25640389 -0.22351672]\n",
      " [-0.71389442  2.23214121]]\n",
      "At step 195250, TRAINING loss is 499.2509765939817\n",
      "At step 195500, TRAINING loss is 121.19398248039107\n",
      "At step 195750, TRAINING loss is 1834.1105167398835\n",
      "At step 196000, TRAINING loss is 935.7560415958176\n",
      "At step 196000, sampled x_0 is [[1.51111711 0.13844535]\n",
      " [1.50788438 1.69148576]]\n",
      "At step 196250, TRAINING loss is 2945.4886416215522\n",
      "At step 196500, TRAINING loss is 451.88866423408604\n",
      "At step 196750, TRAINING loss is 3538.52804576411\n",
      "At step 197000, TRAINING loss is 4516.440692306441\n",
      "At step 197000, sampled x_0 is [[-0.03060185  0.43508455]\n",
      " [ 2.54712666 -1.20629677]]\n",
      "At step 197250, TRAINING loss is 1090.5379124394167\n",
      "At step 197500, TRAINING loss is 4205.780272512162\n",
      "At step 197750, TRAINING loss is 3180.5494551926568\n",
      "At step 198000, TRAINING loss is 1900.287128830159\n",
      "At step 198000, sampled x_0 is [[ 0.3923684  -0.05905953]\n",
      " [ 0.92426908  1.07331901]]\n",
      "At step 198250, TRAINING loss is 305.79525730360353\n",
      "At step 198500, TRAINING loss is 619.3118710037081\n",
      "At step 198750, TRAINING loss is 4428.676872409667\n",
      "At step 199000, TRAINING loss is 3504.5295811026062\n",
      "At step 199000, sampled x_0 is [[-0.56942043 -1.81304377]\n",
      " [-1.41276942  0.55403635]]\n",
      "At step 199250, TRAINING loss is 331.819483607186\n",
      "At step 199500, TRAINING loss is 4845.669706672179\n",
      "At step 199750, TRAINING loss is 537.0725784270398\n",
      "At step 200000, TRAINING loss is 3470.8577841006463\n",
      "At step 200000, sampled x_0 is [[-1.728152   -1.00129945]\n",
      " [ 1.8957372   1.05075668]]\n",
      "At step 200250, TRAINING loss is 4629.978204144854\n",
      "At step 200500, TRAINING loss is 5.949843907453882\n",
      "At step 200750, TRAINING loss is 1669.3926322671882\n",
      "At step 201000, TRAINING loss is 2088.864894966346\n",
      "At step 201000, sampled x_0 is [[0.87467238 1.19408834]\n",
      " [1.74732715 0.95116393]]\n",
      "At step 201250, TRAINING loss is 1891.676591164232\n",
      "At step 201500, TRAINING loss is 94.05421367992615\n",
      "At step 201750, TRAINING loss is 418.9223643528545\n",
      "At step 202000, TRAINING loss is 3152.183735156664\n",
      "At step 202000, sampled x_0 is [[-1.75391376  0.74848502]\n",
      " [ 0.48465046  0.62851994]]\n",
      "At step 202250, TRAINING loss is 579.3460344305647\n",
      "At step 202500, TRAINING loss is 69.53952195505417\n",
      "At step 202750, TRAINING loss is 4707.781041495466\n",
      "At step 203000, TRAINING loss is 2346.3596374474705\n",
      "At step 203000, sampled x_0 is [[-0.99593183  0.89457667]\n",
      " [ 0.56534026  1.01577501]]\n",
      "At step 203250, TRAINING loss is 241.32323049260577\n",
      "At step 203500, TRAINING loss is 3566.216444457804\n",
      "At step 203750, TRAINING loss is 4889.98594732382\n",
      "At step 204000, TRAINING loss is 3181.657114955196\n",
      "At step 204000, sampled x_0 is [[1.75418313 1.33866321]\n",
      " [2.81534985 0.72907249]]\n",
      "At step 204250, TRAINING loss is 2121.4924331449324\n",
      "At step 204500, TRAINING loss is 202.81998649223556\n",
      "At step 204750, TRAINING loss is 2968.590982087847\n",
      "At step 205000, TRAINING loss is 986.6586003937398\n",
      "At step 205000, sampled x_0 is [[ 0.68464098 -0.18909112]\n",
      " [ 0.12046648  0.03938161]]\n",
      "At step 205250, TRAINING loss is 3173.0556250934583\n",
      "At step 205500, TRAINING loss is 110.23405542949786\n",
      "At step 205750, TRAINING loss is 16.63655153392918\n",
      "At step 206000, TRAINING loss is 375.10480084832136\n",
      "At step 206000, sampled x_0 is [[ 0.49476289  2.24761682]\n",
      " [ 1.41090528 -0.54341986]]\n",
      "At step 206250, TRAINING loss is 655.6750636988684\n",
      "At step 206500, TRAINING loss is 31.1182400296862\n",
      "At step 206750, TRAINING loss is 1365.4460256976342\n",
      "At step 207000, TRAINING loss is 63.3619674192703\n",
      "At step 207000, sampled x_0 is [[-0.33275307 -0.46144338]\n",
      " [ 2.16360806  0.36590083]]\n",
      "At step 207250, TRAINING loss is 187.34165131403748\n",
      "At step 207500, TRAINING loss is 2279.2182820242615\n",
      "At step 207750, TRAINING loss is 1864.4453982022371\n",
      "At step 208000, TRAINING loss is 960.6287131314357\n",
      "At step 208000, sampled x_0 is [[2.03890224 0.45170191]\n",
      " [1.73459518 0.26940799]]\n",
      "At step 208250, TRAINING loss is 522.2606587233078\n",
      "At step 208500, TRAINING loss is 4274.234707348873\n",
      "At step 208750, TRAINING loss is 3093.4551844638645\n",
      "At step 209000, TRAINING loss is 2336.673829747745\n",
      "At step 209000, sampled x_0 is [[ 0.63474172 -0.22049615]\n",
      " [ 1.15213742  1.83507509]]\n",
      "At step 209250, TRAINING loss is 3256.3945645153444\n",
      "At step 209500, TRAINING loss is 842.7459338885783\n",
      "At step 209750, TRAINING loss is 3884.6084416956073\n",
      "At step 210000, TRAINING loss is 1370.1355245486102\n",
      "At step 210000, sampled x_0 is [[1.68712638 0.31311676]\n",
      " [1.54787155 0.02662043]]\n",
      "At step 210250, TRAINING loss is 18.89681495632948\n",
      "At step 210500, TRAINING loss is 28.083951932235784\n",
      "At step 210750, TRAINING loss is 3924.2002786574885\n",
      "At step 211000, TRAINING loss is 205.03553606600258\n",
      "At step 211000, sampled x_0 is [[0.87389614 1.59770708]\n",
      " [0.4685203  2.4250397 ]]\n",
      "At step 211250, TRAINING loss is 348.71668055850284\n",
      "At step 211500, TRAINING loss is 1518.130230313045\n",
      "At step 211750, TRAINING loss is 2302.6891008494167\n",
      "At step 212000, TRAINING loss is 2753.092971563231\n",
      "At step 212000, sampled x_0 is [[-2.09006014 -0.09747345]\n",
      " [ 2.67666957  1.56234605]]\n",
      "At step 212250, TRAINING loss is 3636.084683705425\n",
      "At step 212500, TRAINING loss is 1499.7375363307046\n",
      "At step 212750, TRAINING loss is 2.642646307061068\n",
      "At step 213000, TRAINING loss is 304.555763905008\n",
      "At step 213000, sampled x_0 is [[0.35415342 0.36850898]\n",
      " [1.19276028 0.75884538]]\n",
      "At step 213250, TRAINING loss is 1130.1053903848485\n",
      "At step 213500, TRAINING loss is 2317.8232303222094\n",
      "At step 213750, TRAINING loss is 1008.3128792373457\n",
      "At step 214000, TRAINING loss is 794.9733703410268\n",
      "At step 214000, sampled x_0 is [[-0.12481761 -0.28265381]\n",
      " [-0.0831357   2.03092818]]\n",
      "At step 214250, TRAINING loss is 97.441225082979\n",
      "At step 214500, TRAINING loss is 428.2549496356179\n",
      "At step 214750, TRAINING loss is 2534.515864821237\n",
      "At step 215000, TRAINING loss is 4236.505373075514\n",
      "At step 215000, sampled x_0 is [[0.1053227  0.78172179]\n",
      " [1.17275479 2.1061423 ]]\n",
      "At step 215250, TRAINING loss is 4705.016949779668\n",
      "At step 215500, TRAINING loss is 582.8429728262681\n",
      "At step 215750, TRAINING loss is 197.92109811174728\n",
      "At step 216000, TRAINING loss is 1537.7823460460738\n",
      "At step 216000, sampled x_0 is [[0.76180035 1.05833655]\n",
      " [1.41107673 0.10829062]]\n",
      "At step 216250, TRAINING loss is 3311.860964255207\n",
      "At step 216500, TRAINING loss is 880.1091598408286\n",
      "At step 216750, TRAINING loss is 11.285609330679172\n",
      "At step 217000, TRAINING loss is 543.1560566049753\n",
      "At step 217000, sampled x_0 is [[-0.0373876   0.15138882]\n",
      " [ 1.06244955 -0.16302392]]\n",
      "At step 217250, TRAINING loss is 181.44142630945208\n",
      "At step 217500, TRAINING loss is 10.056687987510198\n",
      "At step 217750, TRAINING loss is 3200.190717758762\n",
      "At step 218000, TRAINING loss is 3562.035244615921\n",
      "At step 218000, sampled x_0 is [[0.60931954 0.40277816]\n",
      " [1.8154514  0.31964582]]\n",
      "At step 218250, TRAINING loss is 3504.1507869273637\n",
      "At step 218500, TRAINING loss is 210.62603390204197\n",
      "At step 218750, TRAINING loss is 2742.081605200589\n",
      "At step 219000, TRAINING loss is 4669.254134711763\n",
      "At step 219000, sampled x_0 is [[-0.57828852  0.11376711]\n",
      " [ 0.27318823  0.04883735]]\n",
      "At step 219250, TRAINING loss is 44.74313363169824\n",
      "At step 219500, TRAINING loss is 1487.6085963143548\n",
      "At step 219750, TRAINING loss is 2239.2884266305027\n",
      "At step 220000, TRAINING loss is 3013.6007638708325\n",
      "At step 220000, sampled x_0 is [[ 0.22857853 -1.03778396]\n",
      " [ 2.35128374 -1.02364325]]\n",
      "At step 220250, TRAINING loss is 26.356135717109076\n",
      "At step 220500, TRAINING loss is 8.193302078302446\n",
      "At step 220750, TRAINING loss is 383.7790502156287\n",
      "At step 221000, TRAINING loss is 4741.80732336178\n",
      "At step 221000, sampled x_0 is [[ 0.12407993 -0.46420325]\n",
      " [ 0.4195493   1.00351305]]\n",
      "At step 221250, TRAINING loss is 796.2960494484427\n",
      "At step 221500, TRAINING loss is 4088.9366130191343\n",
      "At step 221750, TRAINING loss is 1769.373240915252\n",
      "At step 222000, TRAINING loss is 111.25659570543151\n",
      "At step 222000, sampled x_0 is [[-0.93708985 -0.30712205]\n",
      " [ 1.04996593  1.86645128]]\n",
      "At step 222250, TRAINING loss is 499.7793983609712\n",
      "At step 222500, TRAINING loss is 2734.650572672148\n",
      "At step 222750, TRAINING loss is 445.81167546519885\n",
      "At step 223000, TRAINING loss is 183.0808409400677\n",
      "At step 223000, sampled x_0 is [[0.52059691 2.28450994]\n",
      " [1.63983976 1.42513021]]\n",
      "At step 223250, TRAINING loss is 53.49758436782233\n",
      "At step 223500, TRAINING loss is 45.71571189256156\n",
      "At step 223750, TRAINING loss is 3216.9722773567264\n",
      "At step 224000, TRAINING loss is 4478.093899429077\n",
      "At step 224000, sampled x_0 is [[0.76019706 1.20552742]\n",
      " [3.17254916 0.9097121 ]]\n",
      "At step 224250, TRAINING loss is 613.2562008507459\n",
      "At step 224500, TRAINING loss is 632.2485670559413\n",
      "At step 224750, TRAINING loss is 3757.533972532883\n",
      "At step 225000, TRAINING loss is 13.996143236047356\n",
      "At step 225000, sampled x_0 is [[-0.77179385  0.72136261]\n",
      " [ 0.43055119  1.57957075]]\n",
      "At step 225250, TRAINING loss is 3632.5644846834302\n",
      "At step 225500, TRAINING loss is 3487.321371534839\n",
      "At step 225750, TRAINING loss is 1270.4713970984171\n",
      "At step 226000, TRAINING loss is 390.48664925578373\n",
      "At step 226000, sampled x_0 is [[-0.99395768  0.2783401 ]\n",
      " [ 0.56363022  0.31465337]]\n",
      "At step 226250, TRAINING loss is 2586.8177644286297\n",
      "At step 226500, TRAINING loss is 3064.5948015901836\n",
      "At step 226750, TRAINING loss is 1483.0501704122512\n",
      "At step 227000, TRAINING loss is 2689.2672578021975\n",
      "At step 227000, sampled x_0 is [[ 1.00259706 -0.42682109]\n",
      " [-0.01597674 -0.68909697]]\n",
      "At step 227250, TRAINING loss is 699.058174534341\n",
      "At step 227500, TRAINING loss is 1982.268993477639\n",
      "At step 227750, TRAINING loss is 343.00082820516144\n",
      "At step 228000, TRAINING loss is 4203.95681933774\n",
      "At step 228000, sampled x_0 is [[-0.94464194 -0.45636461]\n",
      " [ 0.3534846   1.0872839 ]]\n",
      "At step 228250, TRAINING loss is 352.07192495386835\n",
      "At step 228500, TRAINING loss is 4095.487564498694\n",
      "At step 228750, TRAINING loss is 6.241630579935015\n",
      "At step 229000, TRAINING loss is 2780.7168420725648\n",
      "At step 229000, sampled x_0 is [[-1.12650311  1.32828219]\n",
      " [ 0.39071653  0.41778965]]\n",
      "At step 229250, TRAINING loss is 106.84085478010341\n",
      "At step 229500, TRAINING loss is 226.53744794532264\n",
      "At step 229750, TRAINING loss is 3095.3457192352694\n",
      "At step 230000, TRAINING loss is 2121.7391034476277\n",
      "At step 230000, sampled x_0 is [[-0.94377656 -0.5029494 ]\n",
      " [ 1.94994277  1.16939605]]\n",
      "At step 230250, TRAINING loss is 1356.723084837865\n",
      "At step 230500, TRAINING loss is 224.61823499220938\n",
      "At step 230750, TRAINING loss is 3273.512959290875\n",
      "At step 231000, TRAINING loss is 32.07955252414795\n",
      "At step 231000, sampled x_0 is [[1.18193286 0.31940775]\n",
      " [1.07458981 2.73143071]]\n",
      "At step 231250, TRAINING loss is 4609.370647887022\n",
      "At step 231500, TRAINING loss is 3976.62606506115\n",
      "At step 231750, TRAINING loss is 1066.555524407392\n",
      "At step 232000, TRAINING loss is 2775.258057756374\n",
      "At step 232000, sampled x_0 is [[-0.46543675 -0.07099864]\n",
      " [ 1.79495971  0.51566027]]\n",
      "At step 232250, TRAINING loss is 4913.1615269183785\n",
      "At step 232500, TRAINING loss is 1423.1976766880557\n",
      "At step 232750, TRAINING loss is 1585.4541234153112\n",
      "At step 233000, TRAINING loss is 3867.1406462355535\n",
      "At step 233000, sampled x_0 is [[ 1.42971302 -0.68813498]\n",
      " [-0.05430762  0.09847653]]\n",
      "At step 233250, TRAINING loss is 2674.8375672858524\n",
      "At step 233500, TRAINING loss is 462.7859501370468\n",
      "At step 233750, TRAINING loss is 2354.0600150786368\n",
      "At step 234000, TRAINING loss is 7.9516678166828365\n",
      "At step 234000, sampled x_0 is [[-0.70954412  0.44013912]\n",
      " [ 1.57453557 -1.31075798]]\n",
      "At step 234250, TRAINING loss is 108.20738933613072\n",
      "At step 234500, TRAINING loss is 4663.724227037097\n",
      "At step 234750, TRAINING loss is 2335.5513860597134\n",
      "At step 235000, TRAINING loss is 4634.383209627491\n",
      "At step 235000, sampled x_0 is [[-0.75347517  1.45118434]\n",
      " [ 0.59273668 -0.15031958]]\n",
      "At step 235250, TRAINING loss is 69.23237931930888\n",
      "At step 235500, TRAINING loss is 1004.957282164911\n",
      "At step 235750, TRAINING loss is 2656.0436633860722\n",
      "At step 236000, TRAINING loss is 97.83665498612376\n",
      "At step 236000, sampled x_0 is [[-0.55492762  1.31105221]\n",
      " [ 0.90832036  0.58197238]]\n",
      "At step 236250, TRAINING loss is 1598.4433758407063\n",
      "At step 236500, TRAINING loss is 1793.9394791115915\n",
      "At step 236750, TRAINING loss is 290.5298355003488\n",
      "At step 237000, TRAINING loss is 201.15627704608158\n",
      "At step 237000, sampled x_0 is [[0.32505275 1.55488047]\n",
      " [1.9407916  2.74199767]]\n",
      "At step 237250, TRAINING loss is 1021.7264229369289\n",
      "At step 237500, TRAINING loss is 3688.601835743827\n",
      "At step 237750, TRAINING loss is 4124.329597075397\n",
      "At step 238000, TRAINING loss is 1519.4378786174593\n",
      "At step 238000, sampled x_0 is [[-0.31815429 -0.35231771]\n",
      " [-0.37430948  2.20939689]]\n",
      "At step 238250, TRAINING loss is 4432.391708130289\n",
      "At step 238500, TRAINING loss is 4516.910051095276\n",
      "At step 238750, TRAINING loss is 3625.502601907422\n",
      "At step 239000, TRAINING loss is 480.9337591611825\n",
      "At step 239000, sampled x_0 is [[ 0.23254435 -1.099631  ]\n",
      " [ 1.35499794  1.47715079]]\n",
      "At step 239250, TRAINING loss is 14.186167981091181\n",
      "At step 239500, TRAINING loss is 2884.472345831435\n",
      "At step 239750, TRAINING loss is 4965.934405756469\n",
      "At step 240000, TRAINING loss is 3068.394167336737\n",
      "At step 240000, sampled x_0 is [[0.94013093 1.10730353]\n",
      " [0.38033667 1.12711687]]\n",
      "At step 240250, TRAINING loss is 4246.0144090714475\n",
      "At step 240500, TRAINING loss is 83.88406305425988\n",
      "At step 240750, TRAINING loss is 180.06697660923152\n",
      "At step 241000, TRAINING loss is 2081.5424242857835\n",
      "At step 241000, sampled x_0 is [[ 1.52166603 -0.15520098]\n",
      " [ 2.22821454 -0.60863506]]\n",
      "At step 241250, TRAINING loss is 3907.7141864075475\n",
      "At step 241500, TRAINING loss is 15.89415469657283\n",
      "At step 241750, TRAINING loss is 2894.9315866826937\n",
      "At step 242000, TRAINING loss is 182.57962249514088\n",
      "At step 242000, sampled x_0 is [[-0.50462624  0.77594619]\n",
      " [ 0.05140704  2.54880734]]\n",
      "At step 242250, TRAINING loss is 448.59535643395327\n",
      "At step 242500, TRAINING loss is 1611.462775600819\n",
      "At step 242750, TRAINING loss is 4966.900524890561\n",
      "At step 243000, TRAINING loss is 3531.001825217807\n",
      "At step 243000, sampled x_0 is [[-0.46050573  0.08937934]\n",
      " [ 0.86323855  1.7728906 ]]\n",
      "At step 243250, TRAINING loss is 277.8638239022463\n",
      "At step 243500, TRAINING loss is 3356.345629483182\n",
      "At step 243750, TRAINING loss is 929.4845014944549\n",
      "At step 244000, TRAINING loss is 4019.337363487095\n",
      "At step 244000, sampled x_0 is [[-2.26569469  0.44186339]\n",
      " [-0.46489214  1.93461327]]\n",
      "At step 244250, TRAINING loss is 2851.9809900449986\n",
      "At step 244500, TRAINING loss is 3802.5679489772174\n",
      "At step 244750, TRAINING loss is 4584.332192074535\n",
      "At step 245000, TRAINING loss is 922.0007414893532\n",
      "At step 245000, sampled x_0 is [[-0.77169086  1.08633942]\n",
      " [ 1.10439114  1.08858621]]\n",
      "At step 245250, TRAINING loss is 4425.064070992549\n",
      "At step 245500, TRAINING loss is 2011.762182177027\n",
      "At step 245750, TRAINING loss is 169.30162714897358\n",
      "At step 246000, TRAINING loss is 10.101073339696583\n",
      "At step 246000, sampled x_0 is [[ 0.40777309  0.93109062]\n",
      " [-0.44040595  2.04072202]]\n",
      "At step 246250, TRAINING loss is 2878.6718913558925\n",
      "At step 246500, TRAINING loss is 4793.999943876342\n",
      "At step 246750, TRAINING loss is 1257.021492081268\n",
      "At step 247000, TRAINING loss is 3884.889454054978\n",
      "At step 247000, sampled x_0 is [[-0.52961717  1.13969884]\n",
      " [ 2.81818896 -0.73133129]]\n",
      "At step 247250, TRAINING loss is 3098.54732004807\n",
      "At step 247500, TRAINING loss is 2729.565347621843\n",
      "At step 247750, TRAINING loss is 639.812279110721\n",
      "At step 248000, TRAINING loss is 314.03048630056537\n",
      "At step 248000, sampled x_0 is [[-0.43050008  0.74349444]\n",
      " [ 2.58158707  1.86719182]]\n",
      "At step 248250, TRAINING loss is 40.253926337378196\n",
      "At step 248500, TRAINING loss is 4363.757519338858\n",
      "At step 248750, TRAINING loss is 24.613054403329464\n",
      "At step 249000, TRAINING loss is 999.4422299311967\n",
      "At step 249000, sampled x_0 is [[ 0.24082944 -0.79444551]\n",
      " [ 0.65633235  0.45326094]]\n",
      "At step 249250, TRAINING loss is 94.2862310668501\n",
      "At step 249500, TRAINING loss is 274.8555572697268\n",
      "At step 249750, TRAINING loss is 3412.4045900408664\n"
     ]
    }
   ],
   "source": [
    "eval_freq=1000\n",
    "for step in ticker:\n",
    "\n",
    "    score_network.train()\n",
    "    \n",
    "    # if DEBUG:\n",
    "    #     inputs = ex_batch\n",
    "    # else:\n",
    "    #     raise NotImplementedError\n",
    "    #     inputs = next(iterator.train)\n",
    "    #     # inputs = inputs.to(device)\n",
    "        \n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    # '''\n",
    "    # Get encoded representation\n",
    "    # '''\n",
    "    \n",
    "    # code = ae.encode(inputs)\n",
    "    \n",
    "    t = rng.uniform(min_t, 1.0)\n",
    "    x_t, gt_score_t = diffuser.forward_marginal(ex_code.detach().cpu().numpy(), t=t)\n",
    "    \n",
    "    score_scaling = torch.tensor(diffuser.score_scaling(t)).to(device)\n",
    "    gt_score_t = torch.tensor(gt_score_t).to(device)\n",
    "    \n",
    "    pred_score_t = score_network(torch.tensor(x_t).float().to(device), t)\n",
    "\n",
    "    score_mse = (gt_score_t - pred_score_t)**2\n",
    "    score_loss = torch.sum(\n",
    "        score_mse / score_scaling[None, None]**2,\n",
    "        dim=(-1, -2)\n",
    "    ) #/ (loss_mask.sum(dim=-1) + 1e-10)    \n",
    "    \n",
    "    # comps = {k: v.mean().item() for k, v in comps._asdict().items()}\n",
    "    score_loss.backward()\n",
    "    optimizer.step()\n",
    "    # check_loss(score_)\n",
    "\n",
    "    if step % config.training.logs_freq == 0:\n",
    "        # log to logger object\n",
    "        # logger.log(\"train\", loss=loss.item(), step=step, **comps)\n",
    "        writer.add_scalar('Training loss', score_loss.item(), global_step=step)\n",
    "        print(f'At step {step}, TRAINING loss is {score_loss.item()}')\n",
    "        \n",
    "    if step % eval_freq == 0:\n",
    "        sampled_x_0 = eval(dt=0.01)\n",
    "        print(f'At step {step}, sampled x_0 is {sampled_x_0[:2, :2]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(score_network, '6.29.23.1D_mlp_score_network.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/rsg/chemistry/johnyang/miniconda3/envs/cellot/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Sampling x_t from learned reverse process, gt x_0 = 1.0, random init')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.scatterplot([x for x in range(len(x_t_list))], x_t_list)\n",
    "plt.xlabel('Time Step (dt = 0.001)')\n",
    "plt.ylabel('x_t Value')\n",
    "plt.title('Sampling x_t from learned reverse process, gt x_0 = 1.0, random init')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
